{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2023/ly-an.chhay/.conda/envs/aggrepred/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from antiberty import AntiBERTyRunner\n",
    "\n",
    "\n",
    "top_folder_path = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "sys.path.insert(0, top_folder_path)\n",
    "\n",
    "from aggrepred.dataset import *\n",
    "from aggrepred.model import *\n",
    "from aggrepred.utils import *\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score,accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AntibodySeqDataset:\n",
    "    def __init__(self, df,  max_seq_len=700):\n",
    "        self.data = df.copy()\n",
    "        \n",
    "        # Convert scores from string to list\n",
    "        self.data['Hchain_scores'] = self.data['Hchain_scores'].apply(ast.literal_eval)\n",
    "        self.data['Lchain_scores'] = self.data['Lchain_scores'].apply(ast.literal_eval)\n",
    "\n",
    "        # Calculate positive and negative counts for heavy and light chains\n",
    "        self.data['Hchain_count_positive'] = self.data['Hchain_scores'].apply(lambda x: sum(1 for score in x if score > 0))\n",
    "        self.data['Hchain_count_negative'] = self.data['Hchain_scores'].apply(lambda x: sum(1 for score in x if score <= 0))\n",
    "        self.data['Lchain_count_positive'] = self.data['Lchain_scores'].apply(lambda x: sum(1 for score in x if score > 0))\n",
    "        self.data['Lchain_count_negative'] = self.data['Lchain_scores'].apply(lambda x: sum(1 for score in x if score <= 0))\n",
    "\n",
    "        # Compute lengths of heavy and light chains\n",
    "        self.data['Hchain_len'] = self.data['Hchain_scores'].apply(len)\n",
    "        self.data['Lchain_len'] = self.data['Lchain_scores'].apply(len)\n",
    "\n",
    "        # Compute negative-to-positive ratio for heavy and light chains\n",
    "        self.data['Hchain_neg_to_pos_ratio'] = self.data['Hchain_count_negative'] / self.data['Hchain_count_positive']\n",
    "        self.data['Lchain_neg_to_pos_ratio'] = self.data['Lchain_count_negative'] / self.data['Lchain_count_positive']\n",
    "\n",
    "        # Set max sequence length and scaling flag\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self.data):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        \n",
    "        row = self.data.iloc[idx]\n",
    "        code = row['ID']\n",
    "        H_seq = row['Hchain_sequence']\n",
    "        L_seq = row['Lchain_sequence']\n",
    "        Hchain_scores = row['Hchain_scores']\n",
    "        Lchain_scores = row['Lchain_scores']\n",
    "\n",
    "        # Prepare target vectors for heavy chain\n",
    "        H_y = Hchain_scores[:self.max_seq_len] + [0] * (450 - len(Hchain_scores))\n",
    "        H_y = torch.tensor(H_y)\n",
    "\n",
    "        # Prepare target vectors for light chain\n",
    "        L_y = Lchain_scores[:self.max_seq_len] + [0] * (250 - len(Lchain_scores))\n",
    "        L_y = torch.tensor(L_y)\n",
    "\n",
    "        H_y_bin = (H_y > 0).int()\n",
    "        L_y_bin = (L_y > 0).int()\n",
    "\n",
    "        # Generate binary mask based on sequence length (1 for actual values, 0 for padding)\n",
    "        H_mask = torch.zeros(450, dtype=torch.bool)\n",
    "        H_mask[:len(Hchain_scores)] = True  # Set the first 'len(Hchain_scores)' to 1\n",
    "\n",
    "        L_mask = torch.zeros(250, dtype=torch.bool)\n",
    "        L_mask[:len(Lchain_scores)] = True  # Set the first 'len(Lchain_scores)' to 1\n",
    "\n",
    "        return {\n",
    "            'code': code,\n",
    "            'H_seq': H_seq,\n",
    "            'H_target_reg': H_y,\n",
    "            'H_target_bin': H_y_bin,\n",
    "            'H_mask': H_mask,\n",
    "\n",
    "            'L_seq': L_seq,\n",
    "            'L_target_reg': L_y,\n",
    "            'L_target_bin': L_y_bin,\n",
    "            'L_mask': L_mask  }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/users/eleves-b/2023/ly-an.chhay/main/data/pisces/tm.csv\")\n",
    "# df\n",
    "\n",
    "# train_dataset = AntibodySeqDataset(df[df.split=='train'],700, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(sequence: str, max_length: int = 1000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    One-hot encode an amino acid sequence\n",
    "\n",
    "    :param sequence:   protein sequence\n",
    "    :param max_length: specify the maximum length for protein sequence to use, it helps to have have size in batches\n",
    "\n",
    "    :return: max_length x num_features tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    seqlen = len(sequence)\n",
    "    use_length = max_length\n",
    "    # use_length = min(seqlen,max_length) #variable to len of seq, if want fix_size like 1024, set it to fix\n",
    "    encoded = torch.zeros((use_length, NUM_AMINOS))\n",
    "    for i in range(min(seqlen, max_length)):\n",
    "        aa = sequence[i]\n",
    "        encoded[i][aa2idx.get(aa, NUM_AMINOS-1)] = 1\n",
    "    return encoded\n",
    "\n",
    "def onehot_encode_batch(sequences: list, max_length: int = 1000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    One-hot encode an amino acid sequence\n",
    "\n",
    "    :param sequence:   protein sequence\n",
    "    :param max_length: specify the maximum length for protein sequence to use, it helps to have have size in batches\n",
    "\n",
    "    :return: max_length x num_features tensor\n",
    "    \"\"\"\n",
    "    batch_size = len(sequences)\n",
    "    batch_encoded = torch.zeros((batch_size, max_length, NUM_AMINOS))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        batch_encoded[i] = onehot_encode(seq, max_length)\n",
    "    return batch_encoded\n",
    "\n",
    "def onehot_meiler_encode(sequence: str, max_length: int = 1000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    One-hot encode an amino acid sequence, then concatenate with Meiler features.\n",
    "\n",
    "    :param sequence:   protein sequence\n",
    "    :param max_length: specify the maximum length for protein sequence to use, it helps to have have size in batches\n",
    "\n",
    "    :return: max_length x num_features tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    seqlen = len(sequence)\n",
    "    use_length = max_length\n",
    "    # use_length = min(seqlen,max_length) #variable to len of seq, if want fix_size like 1024, set it to fix\n",
    "    encoded = torch.zeros((use_length, NUM_AMINOS+ NUM_MEILER))\n",
    "    for i in range(min(seqlen, max_length)):\n",
    "        aa = sequence[i]\n",
    "        encoded[i][aa2idx.get(aa, NUM_AMINOS-1)] = 1\n",
    "        encoded[i][-NUM_MEILER:] = MEILER[aa] if aa in MEILER else MEILER[\"X\"]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def onehot_meiler_encode_batch(sequences: list, max_length: int = 1000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    One-hot encode an amino acid sequence, then concatenate with Meiler features.\n",
    "\n",
    "    :param sequence:   protein sequence\n",
    "    :param max_length: specify the maximum length for protein sequence to use, it helps to have have size in batches\n",
    "\n",
    "    :return: max_length x num_features tensor\n",
    "    \"\"\"\n",
    "    batch_size = len(sequences)\n",
    "    batch_encoded = torch.zeros((batch_size, max_length, NUM_AMINOS+ NUM_MEILER))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        batch_encoded[i] = onehot_meiler_encode(seq, max_length)\n",
    "    return batch_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max length of the sequence set to 1000\n",
    "SEQ_MAX_LEN = 1000\n",
    "\n",
    "# 21 amino acids + 7 meiler features\n",
    "INPUT_FEATURES = 28\n",
    "\n",
    "# kernel size as per Parapred\n",
    "KERNEL_SIZE = 11\n",
    "\n",
    "# hidden output chanel of CNN\n",
    "HIDDEN_CHANNELS = 256\n",
    "\n",
    "\n",
    "\n",
    "def generate_mask(input_tensor: torch.Tensor, masks: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate a mask for masked 1D convolution based on a binary mask, including non-consecutive valid positions.\n",
    "\n",
    "    :param input_tensor: an input tensor for convolution (batch_size x features x max_seqlen)\n",
    "    :param masks: a binary mask (batch_size x max_seqlen) indicating valid positions\n",
    "    :return: mask (batch_size x features x max_seqlen)\n",
    "    \"\"\"\n",
    "    batch_size, channels, max_seqlen = input_tensor.shape\n",
    "\n",
    "    # Expand the binary mask to match the input tensor shape (batch_size x features x max_seqlen)\n",
    "    conv_mask = masks.unsqueeze(1).expand(batch_size, channels, max_seqlen)\n",
    "\n",
    "    return conv_mask.to(device=input_tensor.device)\n",
    "\n",
    "class LocalExtractorBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int = SEQ_MAX_LEN,\n",
    "                 output_dim: int = SEQ_MAX_LEN,\n",
    "                 in_channel: int = INPUT_FEATURES,\n",
    "                 out_channel: Optional[int] = None,\n",
    "                 kernel_size: int = KERNEL_SIZE,\n",
    "                 dilation: int = 1,\n",
    "                 stride: int = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Assert same shape\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim if output_dim is None else output_dim\n",
    "\n",
    "        self.in_channels = in_channel\n",
    "        self.out_channel = in_channel if out_channel is None else out_channel\n",
    "\n",
    "\n",
    "        # Determine the padding required for keeping the same sequence length\n",
    "        assert dilation >= 1 and stride >= 1, \"Dilation and stride must be >= 1.\"\n",
    "        self.dilation, self.stride = dilation, stride\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        padding = self.determine_padding(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            self.kernel_size,\n",
    "            padding=padding)\n",
    "\n",
    "        self.BN = nn.BatchNorm1d(out_channel)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, binary_mask) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the LocalExtractorBlock\n",
    "\n",
    "        :param input_tensor: an input tensor of (bsz x features x seqlen) or (bsz x  x seqlen)\n",
    "        :param mask: a boolean tensor of (bsz x 1 x seqlen)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # self.conv.weight = self.conv.weight.to(input_tensor.device)\n",
    "        # self.conv.bias = self.conv.bias.to(input_tensor.device) if self.conv.bias is not None else None\n",
    "    \n",
    "        o = self.conv(input_tensor)\n",
    "        o = self.BN(o)\n",
    "        o = self.leakyrelu(o)\n",
    "        o = self.dropout(o)\n",
    "\n",
    "        #mask to zero-out values beyond the sequence length\n",
    "        mask = generate_mask(o, binary_mask)\n",
    "\n",
    "        return o * mask\n",
    "    \n",
    "    def determine_padding(self, input_shape: int, output_shape: int) -> int:\n",
    "        \"\"\"\n",
    "        Determine the padding required to keep the same length of the sequence before and after convolution.\n",
    "\n",
    "        formula :  L_out = ((L_in + 2 x padding - dilation x (kernel_size - 1) - 1)/stride + 1)\n",
    "\n",
    "        :return: An integer defining the amount of padding required to keep the \"same\" padding effect\n",
    "        \"\"\"\n",
    "        padding = (((output_shape - 1) * self.stride) + 1 - input_shape + (self.dilation * (self.kernel_size - 1))) // 2\n",
    "\n",
    "        # Ensure padding is non-negative and output shape is consistent\n",
    "        assert padding >= 0, f\"Padding must be non-negative but got {padding}.\"\n",
    "        return padding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_attn_mask(batch_size, num_heads, max_length, masks):\n",
    "    \"\"\"\n",
    "    Generate an attention mask from a provided binary mask.\n",
    "\n",
    "    :param batch_size: int, size of the batch.\n",
    "    :param num_heads: int, number of attention heads.\n",
    "    :param max_length: int, maximum sequence length.\n",
    "    :param masks: a binary mask (batch_size x max_length) indicating the valid positions.\n",
    "    :return: expanded mask for multi-head attention (batch_size * num_heads x max_length x max_length)\n",
    "    \"\"\"\n",
    "    # Initialize a 3D attention mask (batch_size x max_length x max_length)\n",
    "    attn_mask = torch.zeros((batch_size, max_length, max_length), dtype=torch.bool)\n",
    "    \n",
    "\n",
    "    # Populate the attention mask based on the input binary masks\n",
    "    for i, mask in enumerate(masks):\n",
    "        # Use the binary mask to determine valid positions\n",
    "        attn_mask[i] = torch.outer(mask, mask)\n",
    "        attn_mask[i].fill_diagonal_(True)\n",
    "    \n",
    "    # Expand the mask for multiple attention heads\n",
    "    attn_mask = attn_mask.unsqueeze(1).expand(-1, num_heads, -1, -1)\n",
    "    \n",
    "    # Reshape to merge batch and head dimensions\n",
    "    attn_mask = attn_mask.reshape(batch_size * num_heads, max_length, max_length)\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "class Att_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True, rnn_dropout=0.2, num_heads=1):\n",
    "        super(Att_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True, dropout=rnn_dropout if num_layers > 1 else 0)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 2 if bidirectional else hidden_size, num_heads=num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, banary_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through BiLSTM with Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # Packed sequences are not necessary since we are using a mask.\n",
    "        h0 = torch.randn(2 * self.num_layers if self.bidirectional else self.num_layers,\n",
    "                        x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.randn(2 * self.num_layers if self.bidirectional else self.num_layers,\n",
    "                        x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward pass through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Apply MultiHeadAttention\n",
    "        mask = generate_attn_mask(x.size(0),self.num_heads, x.size(1), banary_mask).to(device=output.device)\n",
    "        attn_output, attn_weight = self.attention(output, output, output, attn_mask=~mask)\n",
    "\n",
    "        # mask = generate_attn_mask(x.size(0),self.num_heads, x.size(1), lengths).to(x.device)    #(batch_size, max_length, max_length)\n",
    "        # # attn_output, attn_weight = self.attention(output, output, output)\n",
    "        # attn_output, attn_weight = self.attention(output, output, output, attn_mask=~mask)\n",
    "\n",
    "        return attn_output, (hn, cn)\n",
    "    \n",
    "  \n",
    "    \n",
    "class GlobalInformationExtractor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True, rnn_dropout=0.2, num_heads=1):\n",
    "        super(GlobalInformationExtractor, self).__init__()\n",
    "        self.att_bilstm = Att_BiLSTM(input_size, hidden_size, num_layers, bidirectional, rnn_dropout, num_heads)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        output, (hn,cn) = self.att_bilstm(x, lengths)\n",
    "        output = self.leakyrelu(output)\n",
    "        output = self.dropout(output)\n",
    "        return output, (hn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggrepred(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the Aggrepred model using a configuration dictionary.\n",
    "\n",
    "        :param config: A dictionary containing all the parameters for the model.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Unpack the configuration dictionary\n",
    "        # self.pooling = config.get([\"pooling\"], False)\n",
    "        # self.use_local = config[\"use_local\"]\n",
    "        # # self.use_local = config.get(\"use_local\", False)\n",
    "        # self.use_global = config.get(\"use_global\", False)\n",
    "\n",
    "        self.pooling = config.get(\"pooling\", False) \n",
    "        self.use_local = config.get(\"use_local\", True)  # Default to False if not in config\n",
    "        self.use_global = config.get(\"use_global\", True)\n",
    "        \n",
    "        num_localextractor_block = config.get(\"num_localextractor_block\", 3)\n",
    "        input_dim = config.get(\"input_dim\", 1000)\n",
    "        output_dim = config.get(\"output_dim\", 1000)\n",
    "        in_channel = config.get(\"in_channel\", 28)\n",
    "        out_channel = config.get(\"out_channel\", None)\n",
    "        kernel_size = config.get(\"kernel_size\", 23)\n",
    "        dilation = config.get(\"dilation\", 1)\n",
    "        stride = config.get(\"stride\", 1)\n",
    "        \n",
    "        rnn_hid_dim = config.get(\"rnn_hid_dim\", 256)\n",
    "        rnn_layers = config.get(\"rnn_layers\", 1)\n",
    "        bidirectional = config.get(\"bidirectional\", True)\n",
    "        rnn_dropout = config.get(\"rnn_dropout\", 0.2)\n",
    "        attention_heads = config.get(\"attention_heads\", 1)\n",
    "\n",
    "        # assert self.use_local or self.use_global, \"At least one of the local or global information extractor must be used.\"\n",
    "\n",
    "        out_channel = in_channel if out_channel is None else out_channel\n",
    "        \n",
    "        if self.use_local:\n",
    "            assert num_localextractor_block > 0, \"Number of local extractor blocks must be greater than 0.\"\n",
    "            self.local_extractors = nn.ModuleList([\n",
    "                LocalExtractorBlock(\n",
    "                    input_dim=input_dim,\n",
    "                    output_dim=output_dim,\n",
    "                    in_channel=in_channel if i == 0 else out_channel,\n",
    "                    out_channel=out_channel,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation = dilation,\n",
    "                    stride = stride\n",
    "                ) for i in range(num_localextractor_block)\n",
    "            ])\n",
    "            self.residue_map = nn.Linear(in_channel,out_channel)\n",
    "        \n",
    "        if self.use_global:\n",
    "            self.global_extractor = GlobalInformationExtractor(input_size=in_channel, hidden_size=rnn_hid_dim, num_layers=rnn_layers, bidirectional=bidirectional, rnn_dropout=rnn_dropout, num_heads=attention_heads)\n",
    "\n",
    "        \n",
    "        rnn_hid_dim = rnn_hid_dim * 2 if bidirectional else rnn_hid_dim\n",
    "\n",
    "        # if self.use_local:\n",
    "        #     fc_in_dim = out_channel + rnn_hid_dim  if self.use_global else out_channel\n",
    "        # else:\n",
    "        #     fc_in_dim = rnn_hid_dim \n",
    "\n",
    "        if self.use_local and self.use_global:\n",
    "            fc_in_dim = out_channel + rnn_hid_dim\n",
    "        elif self.use_local:\n",
    "            fc_in_dim = out_channel\n",
    "        elif self.use_global:\n",
    "            fc_in_dim = rnn_hid_dim\n",
    "        else:\n",
    "            fc_in_dim = in_channel\n",
    "\n",
    "        # if self.pooling:\n",
    "        #     self.downproject = nn.Linear(fc_in_dim, 4)\n",
    "        #     fc_in_dim = 4* input_dim\n",
    "        \n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Linear(fc_in_dim, fc_in_dim//2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fc_in_dim//2, fc_in_dim//4),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fc_in_dim//4, 1)\n",
    "        )\n",
    "\n",
    "        # self.tm_reg_layer = nn.Sequential(\n",
    "        #     nn.Linear(fc_in_dim, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(256, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(256, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(256, 1)\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, binary_mask) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #### Local Extracted Information\n",
    "        # residual connection following 3 layers of local extractor blocks \n",
    "\n",
    "        #might use skip connection\n",
    "\n",
    "        if self.use_local:\n",
    "            residue = self.residue_map(input_tensor)\n",
    "            o = input_tensor.permute(0, 2, 1)\n",
    "            residue = residue.permute(0, 2, 1)\n",
    "            for extractor in self.local_extractors:\n",
    "                o = extractor(o, binary_mask)\n",
    "                o = o + residue\n",
    "                residue = o\n",
    "\n",
    "            # print('in',input_tensor.size())\n",
    "            # print('out:',o.size())\n",
    "        \n",
    "\n",
    "            local_extracted_info = o.permute(0, 2, 1)\n",
    "\n",
    "            # print('local:',local_extracted_info.size())\n",
    "\n",
    "\n",
    "        #### Local Extracted Information\n",
    "        if self.use_global:\n",
    "            global_extracted_info, (hn,cn) = self.global_extractor(input_tensor, binary_mask)\n",
    "\n",
    "        \n",
    "        # print('global:',global_extracted_info.size())\n",
    "\n",
    "        # # Concatenate the local and global information\n",
    "        # if self.use_local:\n",
    "        #     final_info = torch.cat((local_extracted_info,global_extracted_info), dim=-1) if self.use_global else local_extracted_info\n",
    "        # else:\n",
    "        #     final_info = global_extracted_info\n",
    "\n",
    "        if self.use_local and self.use_global:\n",
    "            final_info = torch.cat((local_extracted_info, global_extracted_info), dim=-1)\n",
    "        elif self.use_local:\n",
    "            final_info = local_extracted_info\n",
    "        elif self.use_global:\n",
    "            final_info = global_extracted_info\n",
    "        else:\n",
    "            final_info = input_tensor\n",
    "\n",
    "        \n",
    "\n",
    "        # #### Pooling for protein-level prediction\n",
    "        # if self.pooling:\n",
    "        #     expanded_mask = binary_mask.unsqueeze(-1).float()  # (batch_size, max_length, 1)\n",
    "\n",
    "\n",
    "        #     # Apply the mask to the output\n",
    "        #     masked_output = final_info * expanded_mask  # (batch_size, max_length, feature_dim)\n",
    "           \n",
    "\n",
    "        #     downproject = self.downproject(masked_output)\n",
    "        #     final_info = downproject.view(downproject.size(0), -1) \n",
    "\n",
    "        #     print(\"final info suze\" ,final_info.size())\n",
    "\n",
    "        # print('concat:',final_info.size())\n",
    "\n",
    "        reg_output = self.reg_layer(final_info)\n",
    "        \n",
    "        return final_info, reg_output\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def clean_output(output_tensor: torch.Tensor, binary_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Clean the output tensor of probabilities to remove the predictions for padded positions using a binary mask.\n",
    "\n",
    "    :param output_tensor: output from the Parapred model; shape: (max_length x 1)\n",
    "    :param binary_mask: binary mask for the sequence; shape: (max_length, ), where True indicates valid positions.\n",
    "\n",
    "    :return: cleaned output tensor; shape: (sum(binary_mask), )\n",
    "    \"\"\"\n",
    "    # Use the binary mask to filter out the padded positions\n",
    "    return output_tensor[binary_mask].view(-1)\n",
    "\n",
    "def clean_output_batch(output_tensor: torch.Tensor, binary_masks: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Clean the output tensor of probabilities to remove the predictions for padded positions in a batch using binary masks.\n",
    "\n",
    "    :param output_tensor: output from the Parapred model; shape: (batch_size, max_length, 1)\n",
    "    :param binary_masks: binary masks for the sequences; shape: (batch_size, max_length), where True indicates valid positions.\n",
    "\n",
    "    :return: cleaned output tensor; shape: (sum of valid positions across the batch, )\n",
    "    \"\"\"\n",
    "    batch_size, max_length, _ = output_tensor.shape\n",
    "    cleaned_outputs = []\n",
    "\n",
    "    # Loop over each sequence in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Use the binary mask to filter out the padded positions for each sequence\n",
    "        cleaned_outputs.append(output_tensor[i][binary_masks[i]].view(-1))\n",
    "\n",
    "    # Concatenate the cleaned outputs from all sequences\n",
    "    return torch.cat(cleaned_outputs, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# DATA\n",
    "# ----------------\n",
    "\n",
    "def custom_collate(batch):\n",
    "    regs_tensor = [item['target_reg'] for item in batch]\n",
    "    max_len = regs_tensor[0].size()[0]\n",
    "    \n",
    "    orig_lens = [item['orig_len'] for item in batch]\n",
    "    max_orig_len = min(max(orig_lens), max_len)  # Ensure max_orig_len is at most max_len\n",
    "    \n",
    "    # print(max_orig_len)\n",
    "    # truncated_encoded_seqs = [item['encoded_seq'][:max_orig_len,:] for item in batch]\n",
    "    codes = [item['code'] for item in batch]\n",
    "    seqs = [item['seq'] for item in batch]\n",
    "    truncated_regs_tensor = [item['target_reg'][ :max_orig_len] for item in batch]\n",
    "    truncated_bins_tensor = [item['target_bin'][:max_orig_len] for item in batch]\n",
    "    \n",
    "    # encoded_seqs_tensor = torch.stack(truncated_encoded_seqs)\n",
    "    target_regs_tensor = torch.stack(truncated_regs_tensor)\n",
    "    target_bins_tensor = torch.stack(truncated_bins_tensor)\n",
    "\n",
    "    return {\n",
    "        'code': codes,\n",
    "        'seq': seqs,\n",
    "        'target_reg': target_regs_tensor,\n",
    "        'target_bin': target_bins_tensor,\n",
    "        'orig_len': torch.tensor(orig_lens)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #########################################################################\n",
    "# #########################################################################\n",
    "# df = pd.read_csv(\"/users/eleves-b/2023/ly-an.chhay/main/data/pisces/data60_fixed_split.csv\")\n",
    "\n",
    "\n",
    "# # train_dataset = SeqDataset(df[df.split=='train'],1000)\n",
    "# # valid_dataset = SeqDataset(df[df.split=='valid'],1000)\n",
    "# # test_dataset = SeqDataset(df[df.split=='test'],1000)\n",
    "\n",
    "# ## smaple down abit for esm\n",
    "# train_dataset = SeqDataset(df[df.split=='train'].sample(frac=0.10, random_state=42),1000)\n",
    "# valid_dataset = SeqDataset(df[df.split=='valid'].sample(frac=0.10, random_state=42),1000)\n",
    "# test_dataset = SeqDataset(df[df.split=='test'],1000)\n",
    "\n",
    "# # train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# # valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "# # test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # ##collate to flexible max len in batch\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antibody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_from_checkpoint(model, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Loads the model and optimizer state from a checkpoint if it exists.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to load the state into.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n",
    "    - checkpoint_path (str): Path to the checkpoint file.\n",
    "    - device (torch.device): Device to which the model should be moved.\n",
    "    \n",
    "    Returns:\n",
    "    - start_epoch (int): The epoch to start training from.\n",
    "    - best_validation_loss (float): The best validation loss recorded in the checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        # print(f'Best validation loss: {best_validation_loss}')\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_validation_loss = float('inf')  # Assuming lower is better for validation loss\n",
    "        print('No checkpoint found.')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return start_epoch, best_validation_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "df = pd.read_csv(\"../data/csv/antibody.csv\")\n",
    "\n",
    "# train_dataset = AntibodySeqDataset(df[df.split=='train'].sample(frac=0.10, random_state=42))\n",
    "# valid_dataset = AntibodySeqDataset(df[df.split=='valid'].sample(frac=0.10, random_state=42))\n",
    "# test_dataset = AntibodySeqDataset(df[df.split=='test'])\n",
    "train_dataset = AntibodySeqDataset(df[df.split=='train'])\n",
    "valid_dataset = AntibodySeqDataset(df[df.split=='valid'])\n",
    "test_dataset = AntibodySeqDataset(df[df.split=='test'])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "\n",
    "# df = pd.read_csv(\"/users/eleves-b/2023/ly-an.chhay/main/data/pisces/tm.csv\")\n",
    "\n",
    "# # train_dataset = AntibodySeqDataset(df[df.split=='train'].sample(frac=0.10, random_state=42))\n",
    "# # valid_dataset = AntibodySeqDataset(df[df.split=='valid'].sample(frac=0.10, random_state=42))\n",
    "# # test_dataset = AntibodySeqDataset(df[df.split=='test'])\n",
    "# train_dataset = AntibodySeqDataset(df[df.split=='train'],700, True)\n",
    "# valid_dataset = AntibodySeqDataset(df[df.split=='test'],700, True)\n",
    "# test_dataset = AntibodySeqDataset(df[df.split=='holdout'],700, True)\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': ['4j8r', '7qny', '6al4', '6hkg', '7kpb', '7kqk', '5jor', '4hzl', '7lop', '2vh5', '7jwp', '5a16', '4hj0', '3thm', '7l7e', '8d3a', '6wo5', '4eow', '2gfb', '6k65', '1nsn', '6mfp', '8ef3', '8dn7', '7zf6', '6mi2', '6bli', '8bbo', '1osp', '6fg2', '6dwi', '6xm2'], 'H_seq': ['VKLQESGGEVVRPGTSVKVSCKASGYAFTNYLIEWVKQRPGQGLEWIGVINPGSGDTNYNEKFKGKATLTADKSSSTAYMQLNSLTSDDSAVYFCARSGAAAPTYYAMDYWGQGVSVTVSSAKTTPPSVYPLAPAAAAANSMVTLGCLVKGYFPEPVTVTWNSGSLSGGVHTFPAVLQSDLYTLSSSVTVPSSTWPSETVTCNVAHPASSTKVDKKIVPR', 'EVQLLESGGDLIQPGGSLRLSCAASGVTVSSNYMSWVRQAPGKGLEWVSIIYPGGSTFYADSVKGRFTISRDNSKNTLYLQMHSLRAEDTAVYYCARDLGSGDMDVWGKGTTVTVSSASTKGPSVFPLAPSSSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKS', 'VQLVQSGAEVKKPGSSVKVSCKASGYAFSSYWMNWVRQAPGQGLEWMGQIWPGDSDTNYAQKFQGRVTITADESTSTAYMELSSLRSEDTAVYYCARRETTTVGRYYYAMDYWGQGTTVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPK', 'EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYGMAWVRQAPGKGLEWVSFISATGLSTYFADSVKGRFTISRDTTKNTLYLQMNSLRADDTAVYFCARMRRTMIAFGGNDFWGQGTLVTVSSASTKGPSVFLGCLVKDYFPEPVTVSWNSGATSGVHTFPAVLQSSGLYSLSSVVTVPYICNVNHKPSNTKVDK', 'DVQLVESGGGLVQPGRSLKLSCAASGFTFSAYYMAWVRQAPTKGLEWVASINYDGANTFYRDSVKGRFTVSRDNARSSLYLQMDSLRSEDTATYYCTTEAYGYNSNWFGYWGQGTLVTVSSAKTTPPSVYPNSMVTLGCLVKGYFPEPVTVTWNSGSLSSGVHTFPAVLQSDLYTLSSSVTVPSSTWPSETVTCNVAHPASSTKVDKKIVPR', 'EVQLLESGGGLVQPGGSLRLSCAASGFTLSSYQMMWVRQAPGKGLEWVAGITGRGGVTGYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKPALDSDQCGFPEAGCIDAWGQGTLVTVSSASTKGPSVFPLAPGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKS', 'EVQLQQSGPELIKPGASVKMSCEASGYIFTEYYIHWVKQIQGRSLEWIGYVHPKTGDVIYNQNFRGKATLTVNRSSNTAYMELHSLTSEDSAVYYCARWDSWGQGTTLTVSSAKTTPPSVYPLAPGCGSSVTLGCLVKGYFPESVTVTWNSGSLSSSVHTFPALLQSGLYTMSSSVTVPSSTWPSQTVTCSVAHPASSTTVDKKLEP', 'EVMLVESGGDLVKPGGSLKLPCAASGFTVSTYAMSWIRQTPEKRLEWVATISSGGSYTYYPDNVKGRFTISRDIAKNTLYLQMSSLRSEDTAMYYCARHPPTVVAGDAMDYWGQGTSVTVSSAKTTPPSVYPLAPGSSMVTLGCLVKGYFPEPVTVTWNSGSLSSGVHTFPAVLQSDLYTLSSSVTVPSSTWPSETVTCNVAHPASSTKVDKKIVPR', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGWINPNSGGTNYAQKFQGRVTMTRDTSISTAYMELSRLRSDDTAVYYCAREVMVRGALPPYGMDVWGQGTTVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC', 'EVQLLESGGGLVQPGGSLRLSAAASGFTFSTFSMNWVRQAPGKGLEWVSYISRTSKTIYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYVARGRFFDYWGQGTLVTVS', 'QSVEESGGGLVTPGTPLTLTCTVSGIDLSRYAMSWVRQAPGKGLEWIGIFGSLGGIFYASWAKGRFTISKTSPTTVDLKITSPTTEDTATYFCARMPYTTDRDFWGPGTLVTVSSASTKGPSVFPLAPGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEP', 'EVKLQESGPGKLQPSQTLSLTCSFSGFSLTTSGIGVGWIRQPSGKGLEWLAHIWWSASKYYNTALKSRLTISKDTSNNQVFLKIASVDTADTATYYCARAYYGNYGGYYFDYWGQGTTLTVSSAKTTAPSVYPLAPVCGDTTGSSVTLGCLVKGYFPEPVTLTWNSGSLSSGVHTFPAVLQSDLYTLSSSVTVTSSTWPSQSITCNVAHPASSTKVDKKIEPRGPT', 'QLQQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLEWMGGIIPTFGTANYAQKFQGRVTITADESTSTAYMELSSLRSEDTAVYYCAQGPIVGAPTDYWGKGTLVTVSSASTKGPSVFPLAPSGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSTQTYICNVNHKPSNTKVDKRV', 'QLQLQESGPGLVKPSETLSLTCTVSGASISANSYYGVWVRQSPGKGLEWVGSIAYRGNSNSGSTYYNPSLKSRATVSVDSSKNQVSLRLTSVTAADTALYYCARRQLLDDGTGYQWAAFDVWGQGTMVTVSSASTKGPSVFPLAPSSGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPK', 'EVQLVQSGPEVKKPGTSVKVSCKASGFTFMSSAVQWVRQARGQRLEWIGWIVIGSGNTNYAQKFQERVTITRDMSTSTAYMELSSLRSEDTAVYYCAAPYCSSISCNDGFDIWGQGTMVTVSSASTKGPSVFPLAPSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEP', 'EVQLLESGGGLVQPGGSLKLSCAASGFAVNNYPMNWVRQAPGKGLEWVSSIIGSGSRASYADSVKGRFTISRDNGKDIVYLQMNNLRADDTALYYCVKDSSTAWWDKPAASEFDFWGQGTMVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPK', 'VQLVQSGAEVKKPGSSVKVSCKASGGTLSSYGISWVRQAPGQGLEWLGGSIPILGTSVYAQKFQGRVTMTADESTSTAHMELSSLRFDDTAIYYCAGVREGMAAISGKNAFDIWGQGTMVTVSSASTKGPSVFPLAPSGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPK', 'QVHLVQSGSELKKPGASVKVSCKASGYSFSRYGIKWVRQAPGQGLEWMGWINTRSGVPAYAQGFTGRFVFSLDTSVDTAFLEISSLKTEDTGIYYCATRPPRFYDKTEYWEDGFDVWGRGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPK', 'DVKLVESGGGLVQPGGSRKLSCAASGFTFSSFGMHWVRQAPEKGLEWVAYISSGSSTIYYADTVKGRFTISRDNPKNTLFLQMTSLRSEDTAMYYCARGDYYGSRGAYWGQGTLVTVSAKTTAPSVYPLAPVCGDTTGSSVTLGCLVKGYFPEPVTLTWNSGSLSSGVHTFPAVLQSDLYTLSSSVTVTSSTWPSQSITCNVAHPASSTKVDKKIEPRG', 'DVQLQQSGPGLVAPSQSLSITCTVSGFSLTDYGVNWVRQSPGKGLEWLGVIWGDGITDYNSALKSRLSVTKDNSKSQVFLKMNSLQSGDSARYYCVTGLFDYWGQGTTLTVSS', 'DVQLQESGPGLVKPSQSLSLTCTVTGYSITSDYAWNWIRQFPGNKLEWMGYITYSGTTSYNPSLKSRISISRDTSKNQFFMQLNSVTTEDTGTFYCTRGNGDWGQGTTLTVSSAKTTPPSVYPLAPGSAAQTNSMVTLGCLVKGYFPEPVTVTWNSGSLSSGVHTFPAVLQSDLYTLSSSVTVPSSPRPSETVTCNVAHPASSTKVDKKI', 'QVQLVQSGAEVQKPGASVKVSCKASGYTFASYDINWVRQATGQGLEWMGWMNPKTGNTGYAQKFQGRVTLTRNTSISTAYMELTSLRSEDTAVYYCATYRIIAAVGYRYFQYWGQGTLVTVSSASTKGPSVFPLAPSSGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPK', 'EVQLVQSGAEVKRPGESLKISCKTSGYSFTSYWISWVRQMPGKGLEWMGAIDPSDSDTRYSPSFQGQVTISADKSISTAYLQWSSLKASDSATYYCAKEGIAARSLDVWGRGVLVTVSSASTKGPSVFPLAPSSSESTAALGCLVKDYFPEPVTVSWNSGSLTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYVCNVNHKPSNTKVDKRVE', 'VQLVESGGGLVQPGGSLRLSCAASGFNFYYSSIHWVRQAPGKGLEWVASISSYYGSTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARETYYTEFSWSYSWGLDYWGQGTLVTVSSASTKGPSVFPLAPSSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC', 'EVQLVESGPEVKKPGTSVKVSCKASGFSFSMSAMQWVRRARGQRLEWIGWIVPGSGNANYAQKFQERVTITRDESTNTGYMELSSLRSEDTAVYYCAAPHCNKTNCYDAFDIWGQGTMVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPK', 'EVQLVQSGAEVKKPGESLRISCKGSGYSFSTYWISWVRQMPGKGLEWMGKIYPGDSYTNYSPSFQGQVTISADKSISTAYLQWSSLKASDTAMYYCARGYGIFDYWGQGTLVTVSSASTKGPSVFPLAPTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPK', 'VQLQESGPRLVKPSETLSLTCTVSGGSTSSYFWNWIRQPPGKGLEWIGYIYGSGSADYNPSLKSRVTISIDTSKTQFSLKLTSVTAADTAVYYCARSGFCSDDACYRRGSWFDPWGQGTLVTVSSASTKGPSVFPLAPSGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPK', 'QVQLVESGPGRVKPSQTLSLTCTVSGDSINSGINYWNWIRQPAGKELEWIGRIFTSGTTHYNPSLKSRVTISVDRSKNEFSLTLNSVTAADTAVYFCGRGGTDDYVDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKSC', 'EVQLQESGPSLVKPSQTLSLTCSVTGEPITSGFWDWIRKFPGNKLEFMGYIRYGGGTYYNPSLKSPISITRDTSKNHYYLQLNSVVTEDTATYYCARSRDYYGSSGFAFWGEGTLVTVSAAKTTPPSVYPLAPGCGDTTGSSVTLGCLVKGYFPESVTVTWNSGSLSSSVHTFPALLQSGLYTMSSSVTVPSSTWPSQTVTCSVAHPASSTTVDKKLE', 'VQLVESGGGLVKPGGSLRLSCAASGFTFSSYSMSWVRQAPGKGLEWVSSISRSTPYIYYADSVKGRFTISRDNAKNSLYLQMNSLRAEDTAVYYCARDLWSPDSNYYDQSAFDIWGQGTMVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEP', 'QVQLQHSGGGLEQPGGSLRISCAASGFTFNTNDMSWVRQAPGKGLQWVSTIIGIDDTTHYADSVRGRFTVSRDTSKNMVYLQMNSLRVEDTALYYCVKNSGIYSFWGQGTLVTVSSASTKGPSVFPLAPSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPK', 'QQLQESGPGLVKPSETLSLTCTVTGFSLSSYTVNWVRQPAGKGLEWIGYISYGGSAYYASWANGRFTISKDSSKNQVSLKLSSVTAADTAVYFCARHMQVGGAPTGSMAAFDPWGPGTLVTVSSASTKGPSVFPLAPSTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEP'], 'H_target_reg': tensor([[ 0.6957, -1.4832,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.9357, -0.9904, -1.2494,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4179, -0.6355,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 1.0715, -0.2516,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.3773, -0.5673, -1.0719,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.6580, -2.0545,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]), 'H_target_bin': tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), 'H_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]]), 'L_seq': ['DIVMTQSHKFMSTSVGDRVSITCKASQVGTALAWYQQKPGQSPKLLIYWASTRHTGVPDRFTGSGSGTDFTLTISNVQSEDLSDYFCQQYSSYPTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSETDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNR', 'VMTQSPSFLSASVGDRVTITCRASQGISSYLAWYQQKPGKAPKLLIQAASTLQSGVPSRFSGSGSGTEFTLTISSLQPEDFATYYCQQLNSYRYTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC', 'DIQLTQSPSFLSASVGDRVTITCKASQSVDYSGDSYLNWYQQKPGKAPKLLIYDASNLVSGVPSRFSGSGSGTEFTLTISSLQPEDFATYYCQQSTENPWTFGGGTKLEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGE', 'EVVMTQSPATLSVSPGEGATLSCRASQSVNTNVAWYQQKPGQAPRLLIYGASTRATGIPARFSGSGSGTEFTLTISTLQSEDFAVYYCQQYSNWPPITFGQGTRLEIKRTVAAPSVFIFPPTASVVCLLNNFYPRENSQESVTEQDSKDSTYSLSSTLT', 'DIQMTQSPASLPASPEEIVTITCQASQDIGNWLSWYQQKPGKSPQLLIYGATSLADGVPSRFSASRSGTQYSLKISRLQVEDFGIFYCLQGQSTPYTFGAGTKLELKRTDAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNE', 'SELTQDPAVSVALGQTVRITCQGDDSYYGWYQQKPGQAPVTVIYGNDNRPSGIPDRFSGSSSGNTASLTITGAQAEDEADYYCGAYDSSGGGGIFGGGTKLTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAP', 'DVLLTQTPLSLPVNLGDQASISCRSSQTILHSDGYTYLEWYLQRPGQSPKLLIYRVYKRFSGIPDRFRGSGSGMDFTLTISGVEAEDLGIYYCFQGSYVPRTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNE', 'DVLMTQTPLSLPVSLGDQASISCRSSQSLVHSDGNTYLEWYLQKPGQSPNLLIYKLSNRFSGVPDRFSGSGSGTDFTLKISRVEAEDLGVYYCFQGSHVPPTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRN', 'EIVLTQSPATLSLSPGERATLSCRASQSVSSYLAWYQQKPGQAPRLLIYDASNRATGIPARFSGSGSGTDFTLTISSLEPEDFAVYYCQQRSNWPPVTFGGGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR', 'IQMTQSPSSLSASVGDRVTITVRASQSISSYLNWYQQKPGEAPKLLIYSASVLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYAQQSVMIPMTFGQGTKVE', 'DIVMTQTPSSTSAAVGGTVTITCQASQSVANNNYLKWYQQKRGQPPKQLIYSVSTLASGVPSRFKGSGSGTQFTLTISDLEADDAATYYCSGYFNNNIGAFGGGTKLEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGE', 'DIVMTQSPASLAVSLGQRATISCRASQSVSTSSYSYMNWYQQKPGQPPKLLIKYASNLESGVPARFSGSGSGTDFTLNIHPLEEEDTATYYCQHSWEIPWTFGGGTKVEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC', 'SYVLTQPPSASGTPGQRVAISCSGSNSNIGSNTVHWYQQLPGAAPKLLIYSNNQRPSGVPDRFSGSNSGTSASLAISRLQSEDEADYYCAAWDDSLNGVVFGGGTKVTVLQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEVEKTVAPTE', 'QSVLTQPPSVSEAPRQTVTISCSGNSSNIGRYPVNWYQQLPGKAPKLLIYSDNLRFSGVPDRFSGSKSGTTASLAIRDLLSEDEADYYCSTWDDTLEGWVFGGGTKVTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAPTE', 'EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPRLLIYGASSRATGIPDRFSGSGSGTDFTLTISRLEPEDFAVYYCQHYGSSRGWTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRG', 'QLVLTQSPSASASLGASVKLTCTLNSGNTNFAVAWHQQQAGKGPRYLMTINSDGRQSRGDGIPDRFSGSTSGADRYLTISSLHFEDEGDYYCQAWTADLHVFGPGTRVAVVGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAPT', 'DIQMTQSPSSLSASVGDRVTITCRASQGISNYLAWYQQKPGKVPKLLIYAASTLQSGVPSRFSGSGYGTEFTLTISSLQPEDVATYYCQQHDNLPLTFGGGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRG', 'SVLTQPPSASGTPGQSVNISCSGSSSNIGNSYVYWYQQLPGTAPKLLIYRNNRRPSGVPDRFSGSKSDTSASLAISGLRSEDEADYYCATWDDSLSGRLFGGGTKLTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAP', 'QIQMTQSPSSLSASLGERVSLTCRASQEISGYLSWLQQKPDGTIKRLIYAASTLDSGVPKRFSGSRSGSDYSLTISSLESEDFADYYCLQYASSPYTFGGGTKLEILRGGAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC', 'DAVVTQESALTTSPGETVTLTCRSSTGAVTTSNYASWVQEKPDHLFTGLIGGTNNRAPGVPARFSGSLIGDKAALTITGAQTEDEAIYFCALWYSNHWVFGGGTKLTVLGGGGGS', 'DIVLTQSPSSLAVSLGQRATISCRASQSVSTSSFRYMHWYQQKPGQPPRLLIKYASNLESGVPARFSGSGSGTDFTLNIHPVEEEDTATYYCQHSWEIPYTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNE', 'DIQLTQSPSSLSASVGDSVTITCRASQGFGNYLAWYQQRPGKVPEVLIYAATTLQSGVPSRFSGSGSGTDFTLTISSLQPEDVATYYCQKYNSAPFTFGQGTRLEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGE', 'SYELTQPPSVSVSPGQTARITCSGDALPKKYAYWFQQKPGQSPVLIIYEDSKRPSGIPERFSGSSSGTVATLTISGAQVEDEADYYCYSTDSSGYHGLFGGGTRLTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVEVAWKADGSAVNAGVETTKPSKQSNNKYAASSYLSLTSDQWKSHKSYSCQVTHEGSTVEKTVAPA', 'IQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQSSIVWEPITFGQGTKVEIKRTVAAPSVFIFPPSDSQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC', 'AIRMTQSPGTLSLSPGERATLSCRASQSVRSSYLAWYQQKPGQAPRLLIYGASTRATGIPDRFSGSGSGTDFILTINRLEPEDLAVYYCQQFGSSPWTFGQGTKVDIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR', 'SYELTQPPSVSVSPGQTASITCSGDNIGDQYAHWYQQKPGQSPVLVIYQDKNRPSGIPERFSGSNSGNTATLTISGTQAMDEADYYCATYTGFGSLAVFGGGTKLTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAP', 'DIQMTQSPSSLSASVGDRVTITCRASQSIDNYLNWYQQKPGKAPKLLIYAASGLQSGVPSRFSGSGSGTEFTLTVSSLHPEDFATYYCQQSYSTLTWTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGE', 'AIQMTQSPSTLSASVGDRVTITCRASQDINSWLAWYQQKPGKAPKLLIYDASSLHSGVPTRFSGSGSGTEFTLTISSLQPDDFASYYCQQYKSYRTFGRGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRG', 'DIQMSQSSSSFSVSLGDRVTITCKASEDIYSRLAWYQQKPGNAPRLLISGATSLETWVPSRFSGSDSGKDYTLSITSLQTEDVATYFCQQYWSPPPTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC', 'SELTQDPAVSVALGQTVRITCQGDSLRSNYASWYQQKPGQAPLLVIYGKNYRPSGIPDRFSGSYSGNTASLTISGAQAEDEADYYCNSRDSSGDHPVVFGGGTNLTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAPT', 'DIQMTQSPATLSVSPGETVTLSCRASQSVRTNVAWYRHKAGQAPMILVSGASTRASGAPARFSGSGYGTEFTLTITSLQSEDFAVYYCLQYNTWPRTFGQGTKVEVKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR', 'EAVLTQSPGTLSLSPGERATLSCQSSQSVYNNNYLSWFQQKPGQPPRLLIYGASTLTSGVPDRFSGSGSGTDFTLTISRLEPEDFAVYYCAGGYSGSSDKYAFGGGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGE'], 'L_target_reg': tensor([[-1.4848,  0.0000,  0.6865,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.2333,  0.0000, -0.3079,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-2.6970,  0.0000, -2.1811,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.5284, -0.6151,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-2.4451,  0.0000, -2.0631,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.5481,  0.0000,  0.8693,  ...,  0.0000,  0.0000,  0.0000]]), 'L_target_bin': tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0]], dtype=torch.int32), 'L_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]])}\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(test_dataloader):\n",
    "    print(batch)\n",
    "    if idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the propostion of pos/neg class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there are about 80% of negative class vs 20% of positive class  , hence 4:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propotion of position and negative class:  0.09087559688180555 0.9091244031181944\n",
      "ratio of position to negative class:  10.00405427103404\n",
      "propotion of position and negative class:  0.08837646378487783 0.9116235362151222\n",
      "ratio of position to negative class:  10.315229838050056\n",
      "propotion of position and negative class:  0.09189772296568413 0.9081022770343159\n",
      "ratio of position to negative class:  9.881662436548224\n"
     ]
    }
   ],
   "source": [
    "sum_one = train_dataset.data['Hchain_count_positive'].sum()\n",
    "sum_zero = train_dataset.data['Hchain_count_negative'].sum()\n",
    "total = train_dataset.data['Hchain_len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)\n",
    "\n",
    "sum_one = valid_dataset.data['Hchain_count_positive'].sum()\n",
    "sum_zero = valid_dataset.data['Hchain_count_negative'].sum()\n",
    "total = valid_dataset.data['Hchain_len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)\n",
    "\n",
    "sum_one = test_dataset.data['Hchain_count_positive'].sum()\n",
    "sum_zero = test_dataset.data['Hchain_count_negative'].sum()\n",
    "total = test_dataset.data['Hchain_len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propotion of position and negative class:  0.05469640667558992 0.94530359332441\n",
      "ratio of position to negative class:  17.282736669176536\n",
      "propotion of position and negative class:  0.05594240179772623 0.9440575982022738\n",
      "ratio of position to negative class:  16.875528541226217\n",
      "propotion of position and negative class:  0.054230682755002625 0.9457693172449974\n",
      "ratio of position to negative class:  17.43974571586512\n"
     ]
    }
   ],
   "source": [
    "sum_one = train_dataset.data['Lchain_count_positive'].sum()\n",
    "sum_zero = train_dataset.data['Lchain_count_negative'].sum()\n",
    "total = train_dataset.data['Lchain_len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)\n",
    "\n",
    "sum_one = valid_dataset.data['Lchain_count_positive'].sum()\n",
    "sum_zero = valid_dataset.data['Lchain_count_negative'].sum()\n",
    "total = valid_dataset.data['Lchain_len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)\n",
    "\n",
    "sum_one = test_dataset.data['Lchain_count_positive'].sum()\n",
    "sum_zero = test_dataset.data['Lchain_count_negative'].sum()\n",
    "total = test_dataset.data['Lchain_len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# PARAM\n",
    "# ----------------\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define the configuration dictionary with all the model parameters\n",
    "# path = \"./weights_antibody/seq/(onehot)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights_antibody/seq/(onehot_meiler)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights_antibody/seq/(esm35M)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights_antibody/seq/(protbert)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights_antibody/seq/(antiberty)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "path = \"./weights_antibody/seq/(antiberty)_(combinedloss)_(none)/\"\n",
    "\n",
    "# path = \"./weights_antibody/seq/(onehot)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights_antibody/seq/(esm35M)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights/seq/(esm)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights_antibody/seq/(onehot_meiler)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "\n",
    "# Define the configuration dictionary with all the model parameters\n",
    "# path_old = \"./weights/seq/(onehot)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path_old = \"./weights/seq/(onehot_meiler)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path_old = \"./weights/seq/(esm35M)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path_old = \"./weights/seq/(esm)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path_old = \"./weights/seq/(protbert)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "\n",
    "\n",
    "#path to Tm\n",
    "# path = \"./weights_tm/seq/(antiberty)_(regloss)_()/\"\n",
    "# path = \"./weights_tm/seq/(onehot_meiler)_(regloss)_(local_5block256dim)_(global_2layer128_4head)/\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"pooling\": False,\n",
    "    \"antibody\": True,\n",
    "    \"use_local\": False,\n",
    "    # \"use_local\": True,\n",
    "    \"use_global\": False,\n",
    "    # \"use_global\": True,\n",
    "    \"num_localextractor_block\": 5,\n",
    "    \"input_dim\": 700,\n",
    "    \"output_dim\": 700,\n",
    "    \"in_channel\": 512,\n",
    "    \"out_channel\": 256,\n",
    "    \"kernel_size\": 23,\n",
    "    \"dilation\": 1,\n",
    "    \"stride\": 1,\n",
    "    \"rnn_hid_dim\": 128,\n",
    "    \"rnn_layers\": 2,\n",
    "    \"bidirectional\": True,\n",
    "    \"rnn_dropout\": 0.2,\n",
    "    \"attention_heads\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"nb_epochs\": 20,\n",
    "    \"encode_mode\" : 'antiberty'\n",
    "}\n",
    "\n",
    "# with open(path_old+'config.json', 'r') as json_file:\n",
    "#     config = json.load(json_file)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Aggrepred(config)\n",
    "model = model.to(device=device)\n",
    "\n",
    "model\n",
    "\n",
    "# Load the model weights from the checkpoint\n",
    "# _, _ = load_model_from_checkpoint(model, path_old + 'model_best.pt', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------\n",
    "#   OPTIMIZER \n",
    "# ----------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "# optimizer = nn.optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "#                                 betas=(0.9, 0.999),\n",
    "#                                 weight_decay=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# LOSS\n",
    "# ----------------\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1.0, lambda_bin=1.0, pos_weight=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_bin = lambda_bin\n",
    "        self.mse_loss = nn.MSELoss()  # Regression Loss (MSE)\n",
    "        \n",
    "        if pos_weight is not None:\n",
    "            # Binary Classification Loss (Weighted BCE with logits)\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        else:\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, outputs, regression_targets):\n",
    "        # Calculate regression loss\n",
    "        reg_loss = self.mse_loss(outputs, regression_targets)\n",
    "        \n",
    "        # Calculate binary classification loss\n",
    "        # Convert regression output to binary labels (logits) for classification\n",
    "        binary_targets = (regression_targets> 0).float()\n",
    "        bin_loss = self.bce_loss(outputs, binary_targets)\n",
    "        \n",
    "        # Combined weighted loss\n",
    "        total_loss = self.lambda_reg * reg_loss + self.lambda_bin * bin_loss\n",
    "        return total_loss\n",
    "\n",
    "mse_loss  = nn.MSELoss()\n",
    "# mse_loss  = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# bce_loss = nn.BCELoss()\n",
    "# bce_loss = nn.BCELoss(weight=class_weights)\n",
    "\n",
    "# class_weights = torch.Tensor([1.0, 12.0]).cuda()\n",
    "pos_class_weights = torch.Tensor([4.0]).to(device)\n",
    "weighted_bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_class_weights)\n",
    "\n",
    "\n",
    "loss_fn = CombinedLoss(lambda_reg=0.7, lambda_bin=0.3, pos_weight=17.0)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable_params, non_trainable_params\n",
    "\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {trainable}\")\n",
    "print(f\"Number of non-trainable parameters: {non_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_esm_batch(batch_sequences, model, alphabet, repr_layer='last'):\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    data = [(\"protein\" + str(i), seq) for i, seq in enumerate(batch_sequences)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
    "\n",
    "    # Get the embeddings from the last layer\n",
    "    last_layer = model.num_layers\n",
    "    token_embeddings = results[\"representations\"][last_layer]\n",
    "    \n",
    "    return token_embeddings\n",
    "\n",
    "def embed_protbert_batch(sequences, model, tokenizer, device='cuda' ):\n",
    "    model.eval()\n",
    "\n",
    "    sequences_w_spaces = [' '.join(list(seq)) for seq in sequences]\n",
    "    processed_sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_w_spaces]\n",
    "\n",
    "    ids = tokenizer.batch_encode_plus(processed_sequences, add_special_tokens=True, pad_to_max_length=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)[0]\n",
    "\n",
    "    return embedding[:,1:-1,:]\n",
    "\n",
    "\n",
    "def embed_antiberty_batch(sequences, model):\n",
    "    \n",
    "    embeddings = model.embed(sequences)\n",
    "    embeddings = [t[1:-1, :] for t in embeddings]  # Removes the first and last rows\n",
    "\n",
    "    # # Pad the trimmed tensors and stack them\n",
    "    embeddings = nn.utils.rnn.pad_sequence(embeddings , batch_first=True)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{minutes}m {seconds} s\" if minutes>0 else f\"{seconds} s\"\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, encode_mode='onehot_meiler', device = 'cuda', printEvery=100):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count_iter = 0\n",
    "    start_time = time.time()\n",
    "    epoch_start_time = start_time\n",
    "    batch_size = dataloader.batch_size\n",
    "    printEvery = printEvery // batch_size if batch_size else 100  # Adjust printEvery based on batch size\n",
    "\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_model = esm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    protbert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "    protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')\n",
    "    \n",
    "    antiberty_model = AntiBERTyRunner()\n",
    "\n",
    "    with tqdm(total=len(dataloader), desc='Training', unit='batch') as pbar:\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "                 \n",
    "            batch_H_sequences = batch['H_seq']\n",
    "            batch_L_sequences = batch['L_seq']\n",
    "\n",
    "            ## different encoding here\n",
    "            if encode_mode == 'esm':\n",
    "                Hchain_x = embed_esm_batch(batch_H_sequences,  esm_model, alphabet).to(device)\n",
    "                Lchain_x = embed_esm_batch(batch_L_sequences,  esm_model, alphabet).to(device)\n",
    "                Hchain_x   = F.pad(Hchain_x, (0, 0, 0, max(450 - Hchain_x.size(1), 0)))\n",
    "                Lchain_x   = F.pad(Lchain_x, (0, 0, 0, max(250 - Lchain_x.size(1), 0)))\n",
    "            \n",
    "            elif encode_mode == 'protbert':\n",
    "                Hchain_x = embed_protbert_batch(batch_H_sequences,  protbert_model, protbert_tokenizer).to(device)\n",
    "                Lchain_x = embed_protbert_batch(batch_L_sequences,  protbert_model, protbert_tokenizer).to(device)\n",
    "                Hchain_x   = F.pad(Hchain_x, (0, 0, 0, max(450 - Hchain_x.size(1), 0)))\n",
    "                Lchain_x   = F.pad(Lchain_x, (0, 0, 0, max(250 - Lchain_x.size(1), 0)))\n",
    "            \n",
    "            elif encode_mode == 'antiberty':\n",
    "                Hchain_x = embed_antiberty_batch(batch_H_sequences,  antiberty_model).to(device)\n",
    "                Lchain_x = embed_antiberty_batch(batch_L_sequences,  antiberty_model).to(device)\n",
    "                Hchain_x   = F.pad(Hchain_x, (0, 0, 0, max(450 - Hchain_x.size(1), 0)))\n",
    "                Lchain_x   = F.pad(Lchain_x, (0, 0, 0, max(250 - Lchain_x.size(1), 0)))\n",
    "                 \n",
    "            elif encode_mode == 'onehot':\n",
    "                Hchain_x = onehot_encode_batch(batch_H_sequences, 450).to(device)\n",
    "                Lchain_x = onehot_encode_batch(batch_L_sequences, 250).to(device)\n",
    "            else:\n",
    "                Hchain_x = onehot_meiler_encode_batch(batch_H_sequences, 450).to(device)\n",
    "                Lchain_x = onehot_meiler_encode_batch(batch_L_sequences, 250).to(device)\n",
    "            \n",
    "            x = torch.cat((Hchain_x, Lchain_x), dim=1)\n",
    "\n",
    "\n",
    "            Hchain_mask = batch['H_mask'].to(device)\n",
    "            Lchain_mask = batch['L_mask'].to(device)\n",
    "\n",
    "            masks = torch.cat((Hchain_mask, Lchain_mask ), dim=1)\n",
    "\n",
    "            \n",
    "            ## convert (bsz,max_len) to  (bsz,max_len,1)\n",
    "            H_y_reg = batch['H_target_reg'].unsqueeze(2).float().to(device)\n",
    "            L_y_reg = batch['L_target_reg'].unsqueeze(2).float().to(device)\n",
    "\n",
    "            # Hchain_y_reg = clean_output_batch(Hchain_y_reg, Hchain_mask)\n",
    "            \n",
    "            y_reg = torch.cat((H_y_reg, L_y_reg ), dim=1)\n",
    "            \n",
    "            y_reg = clean_output_batch(y_reg, masks)\n",
    "        \n",
    "            \n",
    "            \n",
    "            ## prediction\n",
    "            final_info, output_reg = model(x, masks)\n",
    "            \n",
    "            #trim out the padded part\n",
    "            output_reg = clean_output_batch(output_reg, masks)\n",
    "            # print(orig_len.sum())\n",
    "            assert len(output_reg)==len(y_reg) , 'reg output {} and target {} not same length'.format(len(output_reg),len(y_reg))\n",
    "            \n",
    "            \n",
    "            # reg_loss = mse_loss(output_reg, y_reg)\n",
    "            # # bin_loss = weighted_bce_loss(output_reg, y_bin)\n",
    "\n",
    "            current_loss = loss_fn(output_reg, y_reg)\n",
    "\n",
    "            # else:\n",
    "            #     ## prediction\n",
    "            #     y_reg = batch['target'].float().to(device)\n",
    "            #     final_info, output_reg = model(x, masks)\n",
    "\n",
    "            #     current_loss = mse_loss(output_reg.squeeze(-1), y_reg)\n",
    "                \n",
    "            #     print(\"pred:\",output_reg.squeeze(-1) )\n",
    "            #     print(\"tar:\",y_reg )\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            current_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += current_loss.item()\n",
    "            \n",
    "            printEvery = int(1000/x.size(0))\n",
    "            count_iter += 1\n",
    "            if count_iter % printEvery == 0 or idx == len(dataloader) - 1:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / count_iter) * (len(dataloader) - count_iter)\n",
    "                print(f\"Iteration: {count_iter}, Time: {format_time(elapsed_time)}, Remaining: {format_time(remaining_time)}, Training Loss: {total_loss / count_iter:.4f}\")\n",
    "                start_time = time.time()\n",
    "            torch.cuda.empty_cache()\n",
    "            pbar.update(1)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"==> Average Training loss: mse ={total_loss / len(dataloader)}\")\n",
    "    print(f\"==> Epoch Training Time: {format_time(epoch_time)}\")\n",
    "    print(f\"================================================================\\n\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, encode_mode='onehot_meiler', device= 'cuda', mode='valid'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    targets = []\n",
    "    binary_predictions = []\n",
    "    binary_targets = []\n",
    "    orig_lens = []\n",
    "\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_model = esm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    protbert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "    protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')\n",
    "\n",
    "    antiberty_model = AntiBERTyRunner()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(dataloader), unit='batch') as pbar:\n",
    "            for idx, batch in enumerate(dataloader):\n",
    "                    \n",
    "                batch_H_sequences = batch['H_seq']\n",
    "                batch_L_sequences = batch['L_seq']\n",
    "                            \n",
    "\n",
    "                if encode_mode == 'esm':\n",
    "                    Hchain_x = embed_esm_batch(batch_H_sequences,  esm_model, alphabet).to(device)\n",
    "                    Lchain_x = embed_esm_batch(batch_L_sequences,  esm_model, alphabet).to(device)\n",
    "                    H_max_length =  450\n",
    "                    L_max_length = 250\n",
    "                    Hchain_x   = F.pad(Hchain_x, (0, 0, 0, max(H_max_length - Hchain_x.size(1), 0)))\n",
    "                    Lchain_x   = F.pad(Lchain_x, (0, 0, 0, max(L_max_length - Lchain_x.size(1), 0)))\n",
    "                \n",
    "                elif encode_mode == 'protbert':\n",
    "                    Hchain_x = embed_protbert_batch(batch_H_sequences,  protbert_model, protbert_tokenizer).to(device)\n",
    "                    Lchain_x = embed_protbert_batch(batch_L_sequences,  protbert_model, protbert_tokenizer).to(device)\n",
    "                    H_max_length =  450\n",
    "                    L_max_length = 250\n",
    "                    Hchain_x   = F.pad(Hchain_x, (0, 0, 0, max(H_max_length - Hchain_x.size(1), 0)))\n",
    "                    Lchain_x   = F.pad(Lchain_x, (0, 0, 0, max(L_max_length - Lchain_x.size(1), 0)))\n",
    "                    \n",
    "                elif encode_mode == 'antiberty':\n",
    "                    Hchain_x = embed_antiberty_batch(batch_H_sequences,  antiberty_model).to(device)\n",
    "                    Lchain_x = embed_antiberty_batch(batch_L_sequences,  antiberty_model).to(device)\n",
    "                    H_max_length =  450\n",
    "                    L_max_length = 250\n",
    "                    Hchain_x   = F.pad(Hchain_x, (0, 0, 0, max(H_max_length - Hchain_x.size(1), 0)))\n",
    "                    Lchain_x   = F.pad(Lchain_x, (0, 0, 0, max(L_max_length - Lchain_x.size(1), 0)))\n",
    "                    \n",
    "                elif encode_mode == 'onehot':\n",
    "                    Hchain_x = onehot_encode_batch(batch_H_sequences, 450).to(device)\n",
    "                    Lchain_x = onehot_encode_batch(batch_L_sequences, 250).to(device)\n",
    "                else:\n",
    "                    Hchain_x = onehot_meiler_encode_batch(batch_H_sequences, 450).to(device)\n",
    "                    Lchain_x = onehot_meiler_encode_batch(batch_L_sequences, 250).to(device)\n",
    "\n",
    "      \n",
    "                x = torch.cat((Hchain_x, Lchain_x), dim=1)\n",
    "\n",
    "\n",
    "                Hchain_mask = batch['H_mask'].to(device)\n",
    "                Lchain_mask = batch['L_mask'].to(device)\n",
    "\n",
    "                masks = torch.cat((Hchain_mask, Lchain_mask ), dim=1)\n",
    "\n",
    "                # if not config[\"pooling\"]:\n",
    "\n",
    "                ## convert (bsz,max_len) to  (bsz,max_len,1)\n",
    "                H_y_reg = batch['H_target_reg'].unsqueeze(2).float().to(device)\n",
    "                L_y_reg = batch['L_target_reg'].unsqueeze(2).float().to(device)\n",
    "\n",
    "                # Hchain_y_reg = clean_output_batch(Hchain_y_reg, Hchain_mask)\n",
    "                \n",
    "                y_reg = torch.cat((H_y_reg, L_y_reg ), dim=1)\n",
    "                \n",
    "                y_reg = clean_output_batch(y_reg, masks)\n",
    "            \n",
    "                \n",
    "                \n",
    "                ## prediction\n",
    "                final_info, output_reg = model(x, masks)\n",
    "                \n",
    "                #trim out the padded part\n",
    "                output_reg = clean_output_batch(output_reg, masks)\n",
    "                # print(orig_len.sum())\n",
    "                assert len(output_reg)==len(y_reg) , 'reg output {} and target {} not same length'.format(len(output_reg),len(y_reg))\n",
    "                \n",
    "                \n",
    "                # reg_loss = mse_loss(output_reg, y_reg)\n",
    "                # # bin_loss = weighted_bce_loss(output_reg, y_bin)\n",
    "\n",
    "                current_loss = loss_fn(output_reg, y_reg)\n",
    "\n",
    "                # else:\n",
    "                #     ## prediction\n",
    "                #     y_reg = batch['target'].float().to(device)\n",
    "                #     final_info, output_reg = model(x, masks)\n",
    "\n",
    "                #     # print(output_reg.squeeze(-1) )\n",
    "                #     # print(y_reg )\n",
    "\n",
    "                #     current_loss = mse_loss(output_reg.squeeze(-1), y_reg)\n",
    "\n",
    "                total_loss += current_loss.item()\n",
    "\n",
    "                #append to list of all preds\n",
    "                predictions.append(output_reg.cpu().numpy())\n",
    "                targets.append(y_reg.cpu().numpy())\n",
    "\n",
    "                ################################################################################\n",
    "                y_bin = (y_reg.cpu().numpy() > 0).astype(int)\n",
    "                out_bin = (output_reg.cpu().numpy() > 0).astype(int)\n",
    "                ################################################################################\n",
    "\n",
    "                \n",
    "            \n",
    "                binary_predictions.append(out_bin)\n",
    "                binary_targets.append(y_bin)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                pbar.update(1)\n",
    "\n",
    "    all_predictions = np.concatenate(predictions, axis=0).reshape(-1)\n",
    "    all_targets = np.concatenate(targets, axis=0).reshape(-1)\n",
    "\n",
    "    print(\"all pred:\",all_predictions)\n",
    "    print(\"all tar:\",all_targets)\n",
    "\n",
    "    overall_mse = mean_squared_error(all_targets, all_predictions)\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    overall_mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    overall_r2 = r2_score(all_targets, all_predictions)\n",
    "    overall_pcc, _ = pearsonr(all_targets.flatten(), all_predictions.flatten())\n",
    "    overall_spearman, p_value = spearmanr(all_targets, all_predictions)\n",
    "\n",
    "    print(f\"Overall Regression Metrics\")\n",
    "    print(f\"MSE: {overall_mse:.4f}, RMSE: {overall_rmse:.4f}, MAE: {overall_mae:.4f}, R2: {overall_r2:.4f}, PCC: {overall_pcc:.4f}, spear: {overall_spearman:.4f}, P-value: {p_value:.4f}\")\n",
    "    \n",
    "    # metrics = {\n",
    "    #         \"Regression Metrics\": {\n",
    "    #             \"MSE\": round(float(overall_mse), 4),\n",
    "    #             \"RMSE\": round(float(overall_rmse), 4),\n",
    "    #             \"MAE\": round(float(overall_mae), 4),\n",
    "    #             \"R2\": round(float(overall_r2), 4),\n",
    "    #             \"PCC\": round(float(overall_pcc), 4)\n",
    "    #         }}\n",
    "    \n",
    "    # if not pooling:\n",
    "    all_binary_predictions = np.concatenate(binary_predictions, axis=0).reshape(-1)\n",
    "    all_binary_targets = np.concatenate(binary_targets, axis=0).reshape(-1)\n",
    "    \n",
    "    overall_accuracy = accuracy_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_precision = precision_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_recall = recall_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_f1 = f1_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_auc_roc = roc_auc_score(all_binary_targets, all_predictions)\n",
    "    overall_auc_pr = average_precision_score(all_binary_targets, all_predictions)\n",
    "    overall_mcc = matthews_corrcoef(all_binary_targets, all_binary_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Overall classification Metrics\")\n",
    "    print(f\"Acc: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1-Score: {overall_f1:.4f}, AUC-ROC: {overall_auc_roc:.4f}, AUC-PR: {overall_auc_pr:.4f}, MCC: {overall_mcc:.4f}\")  \n",
    "\n",
    "    metrics = {\n",
    "        \"Regression Metrics\": {\n",
    "            \"MSE\": round(float(overall_mse), 4),\n",
    "            \"RMSE\": round(float(overall_rmse), 4),\n",
    "            \"MAE\": round(float(overall_mae), 4),\n",
    "            \"R2\": round(float(overall_r2), 4),\n",
    "            \"PCC\": round(float(overall_pcc), 4)\n",
    "        },\n",
    "        \"Classification Metrics\": {\n",
    "            \"Accuracy\": round(float(overall_accuracy), 4),\n",
    "            \"Precision\": round(float(overall_precision), 4),\n",
    "            \"Recall\": round(float(overall_recall), 4),\n",
    "            \"F1-Score\": round(float(overall_f1), 4),\n",
    "            \"AUC-ROC\": round(float(overall_auc_roc), 4),\n",
    "            \"AUC-PR\": round(float(overall_auc_pr), 4),\n",
    "            \"MCC\": round(float(overall_mcc), 4)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    return total_loss / len(dataloader),metrics, predictions, targets\n",
    "\n",
    "def train_loop(model, optimizer, train_dataloader, valid_dataloader, nb_epochs,  encode_mode='onehot_meiler', device= 'cuda', save_directory='./weights/'):\n",
    "    start_epoch = 1\n",
    "    best_validation_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Paths for saving losses and metrics\n",
    "    loss_output_path = os.path.join(save_directory, 'losses.json')\n",
    "    metric_output_path = os.path.join(save_directory, 'metrics.json')\n",
    "    \n",
    "    # Initialize lists for losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        print(f'Created directory: {save_directory}')\n",
    "\n",
    "    checkpoint_path = os.path.join(save_directory, 'model_last.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        \n",
    "        # # Load losses from the losses.json file if it exists\n",
    "        # if os.path.exists(loss_output_path):\n",
    "        #     with open(loss_output_path, 'r') as f:\n",
    "        #         losses = json.load(f)\n",
    "        #         train_losses = losses.get('train_losses', [])\n",
    "        #         val_losses = losses.get('val_losses', [])\n",
    "        #     print(f'Loaded losses from {loss_output_path}.')\n",
    "        #     print(train_losses)\n",
    "        #     print(val_losses)\n",
    "        # else:\n",
    "        #     print(f'No losses file found at {loss_output_path}.')\n",
    "\n",
    "    else:\n",
    "        print('No checkpoint found. Starting from beginning.')\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    # Load existing losses if available\n",
    "    if os.path.exists(loss_output_path):\n",
    "        with open(loss_output_path, 'r') as json_file:\n",
    "            existing_losses = json.load(json_file)\n",
    "            train_losses = existing_losses.get('train_losses', [])\n",
    "            val_losses = existing_losses.get('val_losses', [])\n",
    "            print(f'Loaded losses from {loss_output_path}.')\n",
    "            print(train_losses)\n",
    "            print(val_losses)\n",
    "\n",
    "    for epoch in range(start_epoch, nb_epochs + 1):\n",
    "        print(\"==================================================================================\")\n",
    "        print(f'                            -----EPOCH {epoch}-----')\n",
    "        print(\"==================================================================================\")\n",
    "        \n",
    "        train_loss = train_epoch(model, optimizer, train_dataloader, encode_mode ,device, printEvery=1000)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        print(\"==========================VALIDATION===============================================\")\n",
    "        val_loss ,metrics, _ , _ = evaluate(model, valid_dataloader,encode_mode, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'==> Epoch {epoch} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_validation_loss:\n",
    "            early_stopping_counter = 0\n",
    "            best_validation_loss = val_loss\n",
    "            best_model_save_path = os.path.join(save_directory, 'model_best.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'validation_accuracy': val_loss,\n",
    "            }, best_model_save_path)\n",
    "            print('\\n')\n",
    "            print(f'Best model checkpoint saved to: {best_model_save_path}')\n",
    "\n",
    "            # Save metrics of the best model\n",
    "            with open(metric_output_path, 'w') as json_file:\n",
    "                json.dump(metrics, json_file, indent=4)\n",
    "        \n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= 5:\n",
    "                print(\"\\n==> Early stopping triggered. No improvement in validation loss for 3 epochs.\")\n",
    "                break\n",
    "\n",
    "        last_model_save_path = os.path.join(save_directory, 'model_last.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'validation_accuracy': val_loss,\n",
    "        }, last_model_save_path)\n",
    "        print(f'Last epoch model saved to: {last_model_save_path}')\n",
    "\n",
    "        # Save updated losses to the JSON file\n",
    "        losses = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }\n",
    "        with open(loss_output_path, 'w') as json_file:\n",
    "            json.dump(losses, json_file, indent=4)\n",
    "        print(f'Losses updated and saved to: {loss_output_path}')\n",
    "        \n",
    "        print(\"==================================================================================\\n\")\n",
    "    \n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path, exist_ok=True)\n",
    "with open(os.path.join(path, \"config.json\"), 'w') as json_file:\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "train_loop(model,optimizer,train_dataloader,valid_dataloader, 50, config['encode_mode'],device,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, metric, preds_val, tar_val = evaluate(model,test_dataloader,config[\"encode_mode\"],device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x = np.hstack([arr.squeeze() for arr in preds_val])  # Squeeze 2D arrays to 1D and concatenate\n",
    "# y = np.hstack(tar_val)  # Concatenate 1D arrays\n",
    "\n",
    "# # Ensure the lengths of x and y are compatible\n",
    "# assert x.shape[0] == y.shape[0], \"The number of elements in x should match the length of y\"\n",
    "\n",
    "# # Plotting the data\n",
    "# plt.scatter(x, y)\n",
    "# plt.xlabel(\"X (from list of 2D arrays)\")\n",
    "# plt.ylabel(\"Y (from list of 1D arrays)\")\n",
    "# plt.title(\"Scatter plot of X vs Y\")\n",
    "# plt.xlim(50,80)\n",
    "# plt.ylim(50,80)\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate Pearson Correlation Coefficient (PCC)\n",
    "# pcc, _ = pearsonr(x, y)\n",
    "# print(f\"Pearson Correlation Coefficient (PCC): {pcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_from_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Loads the model and optimizer state from a checkpoint if it exists.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to load the state into.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n",
    "    - checkpoint_path (str): Path to the checkpoint file.\n",
    "    - device (torch.device): Device to which the model should be moved.\n",
    "    \n",
    "    Returns:\n",
    "    - start_epoch (int): The epoch to start training from.\n",
    "    - best_validation_loss (float): The best validation loss recorded in the checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        # print(f'Best validation loss: {best_validation_loss}')\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_validation_loss = float('inf')  # Assuming lower is better for validation loss\n",
    "        print('No checkpoint found.')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return start_epoch, best_validation_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the configuration dictionary with all the model parameters\n",
    "# path = \"./weights/seq/(onehot)_(regloss)_(global_1layer256_4head)/\"\n",
    "\n",
    "# with open(path+'config.json', 'r') as json_file:\n",
    "#     config = json.load(json_file)\n",
    "\n",
    "# # ----------------\n",
    "# #  MODEL \n",
    "# # ----------------\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of model paths\n",
    "model_paths = [\n",
    "    # \"./weights_antibody/seq/(onehot_meiler)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(onehot)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(protbert)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(esm35M)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(antiberty)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    \n",
    "    # \"./weights_antibody/seq/(esm35M)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(onehot_meiler)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(onehot)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights_antibody/seq/(esm35M)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "    \n",
    "    \"./weights_tm/seq/(antiberty)_(regloss)_()/\"\n",
    "]\n",
    "\n",
    "for path in model_paths:\n",
    "    # Load the config for the current model\n",
    "    with open(path + 'config.json', 'r') as json_file:\n",
    "        config = json.load(json_file)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Aggrepred(config)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    _, _ = load_model_from_checkpoint(model, optimizer, path + 'model_best.pt', device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, metrics, preds, tar = evaluate(model, test_dataloader, config['pooling'], config['encode_mode'] ,device)\n",
    "\n",
    "    # Save metrics of the best model\n",
    "    with open(path + 'result.json', 'w') as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "\n",
    "    print(f\"Processed model in path: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over batches\n",
    "model1.eval()\n",
    "data = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    lengths = [len(seq) for seq in batch]\n",
    "    print(lengths)\n",
    "    lengths = torch.tensor(lengths).to(device)\n",
    "    x = onehot_meiler_encode_batch(batch).to(device)\n",
    "    \n",
    "    _, out, _ = model1(x, lengths)\n",
    "    \n",
    "    for j, sequence in enumerate(batch):\n",
    "        cleaned_output = clean_output(out[j], lengths[j])\n",
    "        for k, value in enumerate(cleaned_output):\n",
    "            data.append([f'protein_{i*32+j+1}', k+1, sequence[k], value.item()])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=['protein_id', 'amino_acid_id', 'amino_acid', 'value'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_protein_values(protein_df):\n",
    "\n",
    "    amino_acid_ids = protein_df['amino_acid_id']\n",
    "    values = protein_df['value']\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(amino_acid_ids, values, linestyle='-', color='#2596be')\n",
    "    \n",
    "    # Add a vertical line at y=0\n",
    "    plt.axhline(y=0, color='black', linestyle='--')\n",
    "    \n",
    "    plt.title(f'Predicted Values ')\n",
    "    plt.xlabel('Amino Acid ID')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "plot_protein_values(df[df[\"protein_id\"]=='protein_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"protein_id\"]=='protein_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"tmp/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1.0, lambda_bin=1.0, pos_weight=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_bin = lambda_bin\n",
    "        self.mse_loss = nn.MSELoss()  # Regression Loss (MSE)\n",
    "        \n",
    "        if pos_weight is not None:\n",
    "            # Binary Classification Loss (Weighted BCE with logits)\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        else:\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, outputs, regression_targets, binary_targets):\n",
    "        # Calculate regression loss\n",
    "        reg_loss = self.mse_loss(outputs, regression_targets)\n",
    "        \n",
    "        # Calculate binary classification loss\n",
    "        # Convert regression output to binary labels (logits) for classification\n",
    "        bin_loss = self.bce_loss(outputs, binary_targets)\n",
    "        \n",
    "        # Combined weighted loss\n",
    "        total_loss = self.lambda_reg * reg_loss + self.lambda_bin * bin_loss\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "outputs = torch.randn(10)  # Raw model outputs (logits or scores)\n",
    "regression_targets = torch.randn(10)  # Regression target values\n",
    "binary_targets = (regression_targets > 0).float()  # Binary targets based on the regression task\n",
    "\n",
    "# Define the combined loss function with custom weights\n",
    "loss_fn = CombinedLoss(lambda_reg=0.7, lambda_bin=0.3, pos_weight=2.0)\n",
    "\n",
    "# Compute the combined loss\n",
    "loss = loss_fn(outputs, regression_targets, binary_targets)\n",
    "print(outputs)\n",
    "print(outputs)\n",
    "print(\"Combined Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCHIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_and_validate(model, optimizer, train_dataloader, valid_dataloader, nb_epochs, device, save_directory='./weights/', printEvery=50):\n",
    "#     start_epoch = 1\n",
    "#     loss = 0\n",
    "#     losses = []\n",
    "#     count_iter = 0\n",
    "#     best_validation_loss = 1000000\n",
    "\n",
    "#     #if saving directory doesn't exist , create one\n",
    "#     if not os.path.exists(save_directory):\n",
    "#         os.makedirs(save_directory)\n",
    "#         print(f'Created directory: {save_directory}')\n",
    "    \n",
    "#     #continue training by loading the last saved model\n",
    "#     checkpoint_path = save_directory+'/model_last.pt' \n",
    "#     if os.path.exists(checkpoint_path):\n",
    "#         checkpoint = torch.load(checkpoint_path)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         start_epoch = checkpoint['epoch'] + 1\n",
    "#         best_validation_loss = checkpoint['validation_accuracy']\n",
    "#         print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "#     else:\n",
    "#         print('No checkpoint found. Starting from beginning.')\n",
    "\n",
    "#     for i in range(start_epoch, nb_epochs):\n",
    "#         print('-----EPOCH{}-----'.format(i))\n",
    "#         model.train()\n",
    "#         time1 = time.time()\n",
    "#         total_len = 0\n",
    "#         for idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "#             x = batch['encoded_seq'].to(device) # already padded\n",
    "#             orig_len = batch['orig_len'].to(device)\n",
    "\n",
    "#             ## convert (bsz,max_len) to  (bsz,max_len,1)\n",
    "#             y_reg = batch['target_reg'].unsqueeze(2).to(device)\n",
    "#             y_bin = batch['target_bin'].unsqueeze(2).float().to(device)\n",
    "            \n",
    "#             y_reg = clean_output_batch(y_reg, orig_len)\n",
    "#             y_bin = clean_output_batch(y_bin, orig_len)\n",
    "\n",
    "#             ## in case that y from dataset is not padded, can squeeze to 1D directly.\n",
    "#             # y_reg = batch['target_reg'].squeeze(-1).to(device)\n",
    "#             # y_bin = batch['target_bin'].squeeze(-1).float().to(device)\n",
    "\n",
    "#             total_len += orig_len.sum().item()\n",
    "\n",
    "\n",
    "#             ### predict \n",
    "#             final_info, output_reg, logit = model(x, orig_len)\n",
    "            \n",
    "#             #trim out the padded part\n",
    "#             output_reg = clean_output_batch(output_reg, orig_len)\n",
    "#             logit = clean_output_batch(logit, orig_len)\n",
    "\n",
    "#             assert len(logit)==len(y_bin) , 'binary output {} and target {} not same length'.format(len(output_bin),len(y_bin))\n",
    "#             assert len(output_reg)==len(y_reg) , 'reg output {} and target {} not same length'.format(len(output_reg),len(y_reg))\n",
    "\n",
    "#             reg_loss = mse_loss(output_reg, y_reg)\n",
    "#             bin_loss = weighted_bce_loss(logit, y_bin)\n",
    "            \n",
    "#             # print(\"batch {} , reg_loss :{}, bin_loss: {}\".format(idx,reg_loss, bin_loss))\n",
    "\n",
    "#             current_loss = reg_loss + bin_loss\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             current_loss.backward()\n",
    "#             optimizer.step()\n",
    "#             loss += current_loss.item()\n",
    "\n",
    "#             count_iter += 1\n",
    "#             if count_iter % printEvery == 0:\n",
    "#                 time2 = time.time()\n",
    "#                 print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter, time2 - time1, loss / printEvery))\n",
    "#                 losses.append(loss)\n",
    "#                 loss = 0\n",
    "#                 time1 = time.time()\n",
    "#         count_iter = 0\n",
    "#         losses.append(loss)\n",
    "#         loss = 0\n",
    "\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         time1 = time.time()\n",
    "#         total_len = 0\n",
    "#         with torch.no_grad():\n",
    "#             for idx, batch in tqdm(enumerate(valid_dataloader)):\n",
    "#                 x = batch['encoded_seq'].to(device) # already padded\n",
    "#                 orig_len = batch['orig_len'].to(device)\n",
    "\n",
    "#                 ## convert (bsz,max_len) to  (bsz,max_len,1)\n",
    "#                 y_reg = batch['target_reg'].unsqueeze(2).to(device)\n",
    "#                 y_bin = batch['target_bin'].unsqueeze(2).float().to(device)\n",
    "                \n",
    "#                 y_reg = clean_output_batch(y_reg, orig_len)\n",
    "#                 y_bin = clean_output_batch(y_bin, orig_len)\n",
    "\n",
    "#                 ## in case that y from dataset is not padded, can squeeze to 1D directly.\n",
    "#                 # y_reg = batch['target_reg'].squeeze(-1).to(device)\n",
    "#                 # y_bin = batch['target_bin'].squeeze(-1).float().to(device)\n",
    "\n",
    "                \n",
    "#                 total_len += orig_len.sum().item()\n",
    "\n",
    "\n",
    "#                 ### predict \n",
    "#                 final_info, output_reg, logit = model(x, orig_len)\n",
    "                \n",
    "#                 #trim out the padded part\n",
    "#                 output_reg = clean_output_batch(output_reg, orig_len)\n",
    "#                 logit = clean_output_batch(logit, orig_len)\n",
    "\n",
    "#                 assert len(logit)==len(y_bin) , 'binary output {} and target {} not same length'.format(len(output_bin),len(y_bin))\n",
    "#                 assert len(output_reg)==len(y_reg) , 'reg output {} and target {} not same length'.format(len(output_reg),len(y_reg))\n",
    "\n",
    "#                 reg_loss = mse_loss(output_reg, y_reg)\n",
    "#                 bin_loss = weighted_bce_loss(logit, y_bin)\n",
    "                \n",
    "#                 current_loss = reg_loss \n",
    "\n",
    "#                 ## old code\n",
    "#                 # x = batch['encoded_seq'].to(device)\n",
    "#                 # y_reg = batch['target_reg'].unsqueeze(2).to(device)\n",
    "#                 # y_bin = batch['target_bin'].unsqueeze(2).float().to(device)\n",
    "#                 # orig_len = batch['orig_len'].to(device)\n",
    "#                 # total_len += orig_len.sum().item()\n",
    "\n",
    "#                 # final_info, reg_output, bin_output = model(x, orig_len)\n",
    "#                 # reg_loss = mse_loss(reg_output, y_reg) / total_len\n",
    "#                 # current_loss = reg_loss\n",
    "\n",
    "\n",
    "#                 val_loss += current_loss.item()\n",
    "#             time2 = time.time()\n",
    "\n",
    "#             print('best_val', best_validation_loss)\n",
    "#             print(\"val loss\", val_loss)\n",
    "\n",
    "#             best_validation_loss = min(best_validation_loss, val_loss)\n",
    "#             # print('best_val after: ', best_validation_loss)\n",
    "\n",
    "#             print('-----EPOCH' + str(i) + '----- done.  Validation loss: ', str(val_loss / len(valid_dataloader)))\n",
    "#             #save model with best valid loss\n",
    "#             if best_validation_loss == val_loss:\n",
    "#                 print('validation loss improved saving checkpoint...')\n",
    "                \n",
    "#                 best_model_save_path = save_directory+'/model_best.pt' \n",
    "#                 # best_model_save_path = os.path.join(save_directory,'/model_best.pt')\n",
    "#                 torch.save({\n",
    "#                     'epoch': i,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'validation_accuracy': val_loss,\n",
    "#                     'loss': loss,\n",
    "#                 }, best_model_save_path)\n",
    "#                 print('Best model checkpoint saved to: {}'.format(best_model_save_path))\n",
    "            \n",
    "#             #save model of last epoch\n",
    "#             last_model_save_path = save_directory+'/model_last.pt' \n",
    "#             torch.save({\n",
    "#                 'epoch': i,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'validation_accuracy': val_loss,\n",
    "#                 'loss': loss,\n",
    "#             }, last_model_save_path)\n",
    "#             print('Last epoch model saved to: {}'.format(last_model_save_path))\n",
    "            \n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fermi_function(s, alpha=0, beta=0.3):\n",
    "    return 1 / (np.exp(-(s - alpha) * beta) + 1)\n",
    "\n",
    "def reverse_fermi_function(y, alpha=0, beta=0.3):\n",
    "    return alpha - np.log(1 / y - 1) / beta\n",
    "\n",
    "def apply_fermi_function(score_list, alpha=0, beta=0.3):\n",
    "    # Apply fermi_function to each score and round to 4 decimal places\n",
    "    return [round(fermi_function(score, alpha, beta), 4) for score in score_list]\n",
    "\n",
    "def apply_reverse_fermi_function(mapped_list, alpha=0, beta=0.3):\n",
    "    # Apply reverse_fermi_function to each mapped score and round to 4 decimal places\n",
    "    return [round(reverse_fermi_function(mapped_score, alpha, beta), 4) for mapped_score in mapped_list]\n",
    "\n",
    "\n",
    "# Generate a range of aggregation risk values from -10 to 10 to better see the full range of the Fermi function\n",
    "s = np.linspace(-8, 8, 500)\n",
    "\n",
    "# Different values of beta to see the effect on the curve\n",
    "betas = [0.1, 0.3, 0.5, 1, 2]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for beta in betas:\n",
    "    fermi_values = fermi_function(s, alpha=0, beta=beta)\n",
    "    plt.plot(s, fermi_values, label=f'beta = {beta}')\n",
    "\n",
    "plt.title('Fermi Function Mapping of Aggregation Risk Values')\n",
    "plt.xlabel('Aggregation Risk (s)')\n",
    "plt.ylabel('Mapped Value (0 to 1)')\n",
    "plt.ylim(-0.05, 1.05)  # Extend y-axis limits to better show the range 0 to 1\n",
    "# plt.xlim(-5, 5)  # Extend x-axis limits to cover the full range of the function\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "def read_fasta(fasta_text):\n",
    "    headers = []\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_text, \"fasta\"):\n",
    "        headers.append(record.id)\n",
    "        sequences.append(str(record.seq))\n",
    "    return headers,sequences\n",
    "\n",
    "fasta_text =\">prot1\\nQPPVPPQRPM\"\n",
    "\n",
    "head, seqs = read_fasta(fasta_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmseqs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
