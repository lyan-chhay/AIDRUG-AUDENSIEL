{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    average_precision_score, \n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import esm\n",
    "\n",
    "# Custom imports from aggrepred package\n",
    "top_folder_path = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "sys.path.insert(0, top_folder_path)\n",
    "from aggrepred.dataset import *\n",
    "from aggrepred.model import *\n",
    "from aggrepred.utils import *\n",
    "\n",
    "# Seed everything function\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset:\n",
    "    def __init__(self, df, max_seq_len=1000):\n",
    "        self.data = df.copy()\n",
    "        \n",
    "        self.data['scores'] = self.data['scores'].apply(ast.literal_eval)\n",
    "        \n",
    "        def count_pos_neg_values(lst):\n",
    "            count_pos = sum(1 for x in lst if x > 0)\n",
    "            count_neg = sum(1 for x in lst if x <= 0)\n",
    "            return count_pos, count_neg\n",
    "\n",
    "        # Apply the function to create new columns\n",
    "        self.data[['count_positive', 'count_negative']] = self.data['scores'].apply(count_pos_neg_values).apply(pd.Series)\n",
    "        self.data['len'] = self.data['scores'].apply(lambda x: len(x))\n",
    "        \n",
    "        self.data['neg_to_pos_ratio'] = self.data['count_negative'] / self.data['count_positive']\n",
    "        self.max_seq_len = max_seq_len\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "   \n",
    "        if idx < 0 or idx >= len(self.data):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        \n",
    "        row = self.data.iloc[idx]\n",
    "        code = row['ID']\n",
    "        seq = row['sequence']\n",
    "        scores = row['scores']\n",
    "        \n",
    "        y  = scores[:self.max_seq_len] + [0] * (self.max_seq_len - len(scores))\n",
    "        y = torch.tensor(y)\n",
    "\n",
    "        y_bin =  (y>0).int()\n",
    "\n",
    "        # Generate binary mask based on sequence length (1 for actual values, 0 for padding)\n",
    "        mask = torch.zeros(self.max_seq_len, dtype=torch.bool)\n",
    "        mask[:len(scores)] = True  # Set the first 'len(Hchain_scores)' to 1\n",
    "\n",
    "        return {\n",
    "            'code': code,\n",
    "            'seq': seq,\n",
    "            'target_reg': y,\n",
    "            'target_bin': y_bin,\n",
    "            'mask': mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to train different model, adjust the path and config here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# PARAM\n",
    "# ----------------\n",
    "\n",
    "## path here is set in format:  type_embed . type_loss . local block info . global block info .  for easily identified each model \n",
    "\n",
    "\n",
    "# Define the configuration dictionary with all the model parameters\n",
    "# path = \"./weights/seq/(esm35M)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights/seq/(esm35M)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights/seq/(esm35M)_(combinedloss)_()/\"\n",
    "# path = \"./weights/seq/(esm)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights/seq/(protbert)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights/seq/(protbert)_(combinedloss)_(none)/\"\n",
    "# path = \"./weights/seq/()_(combinedloss)_(none)/\"\n",
    "path = \"./weights/seq/(onehot_meiler)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "# path = \"./weights/seq/(onehot_meiler)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"antibody\": False,\n",
    "    # \"use_local\": False,\n",
    "    \"use_local\": True,\n",
    "    # \"use_global\": False,\n",
    "    \"use_global\": True,\n",
    "    \"num_localextractor_block\": 3,\n",
    "    \"input_dim\": 1000,\n",
    "    \"output_dim\": 1000,\n",
    "    \"in_channel\": 28,\n",
    "    \"out_channel\": 256,\n",
    "    \"kernel_size\": 23,\n",
    "    \"dilation\": 1,\n",
    "    \"stride\": 1,\n",
    "    \"rnn_hid_dim\": 128,\n",
    "    \"rnn_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"rnn_dropout\": 0.2,\n",
    "    \"attention_heads\": 4,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"nb_epochs\": 20,\n",
    "    \"encode_mode\" : 'onehot_meiler'\n",
    "}\n",
    "\n",
    "# with open(path+'config.json', 'r') as json_file:\n",
    "#     config = json.load(json_file)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "#  MODEL \n",
    "# ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Aggrepred(config)\n",
    "model = model.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 3778561\n",
      "Number of non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------\n",
    "#   OPTIMIZER \n",
    "# ----------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "# optimizer = nn.optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "#                                 betas=(0.9, 0.999),\n",
    "#                                 weight_decay=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# LOSS\n",
    "# ----------------\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1.0, lambda_bin=1.0, pos_weight=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_bin = lambda_bin\n",
    "        self.mse_loss = nn.MSELoss()  # Regression Loss (MSE)\n",
    "        \n",
    "        if pos_weight is not None:\n",
    "            # Binary Classification Loss (Weighted BCE with logits)\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        else:\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, outputs, regression_targets):\n",
    "        # Calculate regression loss\n",
    "        reg_loss = self.mse_loss(outputs, regression_targets)\n",
    "        \n",
    "        # Calculate binary classification loss\n",
    "        # Convert regression output to binary labels (logits) for classification\n",
    "        binary_targets = (regression_targets> 0).float()\n",
    "        bin_loss = self.bce_loss(outputs, binary_targets)\n",
    "        \n",
    "        # Combined weighted loss\n",
    "        total_loss = self.lambda_reg * reg_loss + self.lambda_bin * bin_loss\n",
    "        return total_loss\n",
    "\n",
    "mse_loss  = nn.MSELoss()\n",
    "pos_class_weights = torch.Tensor([4.0]).to(device)\n",
    "weighted_bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_class_weights)\n",
    "\n",
    "\n",
    "combined_loss = CombinedLoss(lambda_reg=0.7, lambda_bin=0.3, pos_weight=4.0)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable_params, non_trainable_params\n",
    "\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {trainable}\")\n",
    "print(f\"Number of non-trainable parameters: {non_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# DATA\n",
    "# ----------------\n",
    "\n",
    "def custom_collate(batch):\n",
    "    regs_tensor = [item['target_reg'] for item in batch]\n",
    "    mask = [item['mask'] for item in batch]\n",
    "    max_len = regs_tensor[0].size()[0]  #1000\n",
    "    \n",
    "    orig_lens = [item['mask'].sum() for item in batch]\n",
    "    max_orig_len = min(max(orig_lens), max_len)  # Ensure max_orig_len is at most max_len\n",
    "    \n",
    "    # print(max_orig_len)\n",
    "    # truncated_encoded_seqs = [item['encoded_seq'][:max_orig_len,:] for item in batch]\n",
    "    codes = [item['code'] for item in batch]\n",
    "    seqs = [item['seq'] for item in batch]\n",
    "    truncated_regs_tensor = [item['target_reg'][ :max_orig_len] for item in batch]\n",
    "    truncated_bins_tensor = [item['target_bin'][:max_orig_len] for item in batch]\n",
    "    truncated_mask_tensor = [item['mask'][ :max_orig_len] for item in batch]\n",
    "    \n",
    "    # encoded_seqs_tensor = torch.stack(truncated_encoded_seqs)\n",
    "    target_regs_tensor = torch.stack(truncated_regs_tensor)\n",
    "    target_bins_tensor = torch.stack(truncated_bins_tensor)\n",
    "    mask_tensor = torch.stack(truncated_mask_tensor)\n",
    "\n",
    "    return {\n",
    "        'code': codes,\n",
    "        'seq': seqs,\n",
    "        'target_reg': target_regs_tensor,\n",
    "        'target_bin': target_bins_tensor,\n",
    "        'mask': mask_tensor\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "df = pd.read_csv(\"../data/csv/data60_fixed_split.csv\")\n",
    "\n",
    "\n",
    "# ## smaple down abit for esm\n",
    "if config['encode_mode'] not in ['onehot', 'onehot_meiler']:\n",
    "    print(\"yes\")\n",
    "    train_dataset = SeqDataset(df[df.split=='train'].sample(frac=0.10, random_state=42),1000)\n",
    "    valid_dataset = SeqDataset(df[df.split=='valid'].sample(frac=0.10, random_state=42),1000)\n",
    "    test_dataset = SeqDataset(df[df.split=='test'],1000)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "else:\n",
    "    train_dataset = SeqDataset(df[df.split=='train'],1000)\n",
    "    valid_dataset = SeqDataset(df[df.split=='valid'],1000)\n",
    "    test_dataset = SeqDataset(df[df.split=='test'],1000)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "##collate to flexible max len in batch\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the propostion of pos/neg class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there are about 80% of negative class vs 20% of positive class  , hence 4:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propotion of position and negative class:  0.19109947183586853 0.8089005281641315\n",
      "ratio of position to negative class:  4.232876838397961\n",
      "propotion of position and negative class:  0.19214283922078573 0.8078571607792142\n",
      "ratio of position to negative class:  4.2044614519874415\n",
      "propotion of position and negative class:  0.1936254280065119 0.8063745719934882\n",
      "ratio of position to negative class:  4.164610920660527\n"
     ]
    }
   ],
   "source": [
    "sum_one = train_dataset.data['count_positive'].sum()\n",
    "sum_zero = train_dataset.data['count_negative'].sum()\n",
    "total = train_dataset.data['len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)\n",
    "\n",
    "sum_one = valid_dataset.data['count_positive'].sum()\n",
    "sum_zero = valid_dataset.data['count_negative'].sum()\n",
    "total = valid_dataset.data['len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)\n",
    "\n",
    "sum_one = test_dataset.data['count_positive'].sum()\n",
    "sum_zero = test_dataset.data['count_negative'].sum()\n",
    "total = test_dataset.data['len'].sum() \n",
    "\n",
    "print(\"propotion of position and negative class: \" , sum_one/total, sum_zero/total)\n",
    "print(\"ratio of position to negative class: \" , sum_zero/sum_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggrepred(\n",
      "  (local_extractors): ModuleList(\n",
      "    (0): LocalExtractorBlock(\n",
      "      (conv): Conv1d(28, 256, kernel_size=(23,), stride=(1,), padding=(11,))\n",
      "      (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1-2): 2 x LocalExtractorBlock(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(23,), stride=(1,), padding=(11,))\n",
      "      (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (residue_map): Linear(in_features=28, out_features=256, bias=True)\n",
      "  (global_extractor): GlobalInformationExtractor(\n",
      "    (att_bilstm): Att_BiLSTM(\n",
      "      (lstm): LSTM(28, 128, batch_first=True, bidirectional=True)\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (reg_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.1)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_time(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{minutes}m {seconds} s\" if minutes>0 else f\"{seconds} s\"\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader,loss_function, encode_mode='onehot_meiler', device = 'cuda', printEvery=100):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count_iter = 0\n",
    "    start_time = time.time()\n",
    "    epoch_start_time = start_time\n",
    "    batch_size = dataloader.batch_size\n",
    "    printEvery = printEvery // batch_size if batch_size else 100  # Adjust printEvery based on batch size\n",
    "\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_model = esm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    protbert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "    protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')\n",
    "\n",
    "    with tqdm(total=len(dataloader), desc='Training', unit='batch') as pbar:\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "                 \n",
    "            batch_sequences = batch['seq']\n",
    "            mask = batch['mask'].to(device)\n",
    "\n",
    "            \n",
    "            ## different encoding here\n",
    "            if encode_mode == 'esm':\n",
    "                x = embed_esm_batch(batch_sequences, esm_model, alphabet).to(device)\n",
    "                x   = F.pad(x, (0, 0, 0, max(1000 - x.size(1), 0)))\n",
    "            elif encode_mode == 'protbert':\n",
    "                x = embed_protbert_batch(batch_sequences, protbert_model, protbert_tokenizer).to(device)\n",
    "                x   = F.pad(x, (0, 0, 0, max(1000 - x.size(1), 0)))\n",
    "            elif encode_mode == 'onehot':\n",
    "                x = onehot_encode_batch(batch_sequences,1000).to(device)\n",
    "            else:\n",
    "                x = onehot_meiler_encode_batch(batch_sequences,1000).to(device)\n",
    "\n",
    "\n",
    "            ## convert (bsz,max_len) to  (bsz,max_len,1)\n",
    "            y_reg = batch['target_reg'].unsqueeze(2).float().to(device)\n",
    "            y_bin = batch['target_bin'].unsqueeze(2).float().to(device)\n",
    "\n",
    "            \n",
    "            y_reg = clean_output_batch(y_reg, mask)\n",
    "            y_bin = clean_output_batch(y_bin, mask)\n",
    "\n",
    "            ### predict \n",
    "            final_info, output_reg = model(x, mask)\n",
    "            \n",
    "            #trim out the padded part\n",
    "            output_reg = clean_output_batch(output_reg, mask)\n",
    "            # print(orig_len.sum())\n",
    "            assert len(output_reg)==len(y_reg) , 'reg output {} and target {} not same length'.format(len(output_reg),len(y_reg))\n",
    "\n",
    "            # current_loss = reg_loss \n",
    "            current_loss = loss_function(output_reg, y_reg)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            current_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += current_loss.item()\n",
    "            \n",
    "            printEvery = int(1000/x.size(0))\n",
    "            count_iter += 1\n",
    "            if count_iter % printEvery == 0 or idx == len(dataloader) - 1:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / count_iter) * (len(dataloader) - count_iter)\n",
    "                print(f\"Iteration: {count_iter}, Time: {format_time(elapsed_time)}, Remaining: {format_time(remaining_time)}, Training Loss: {total_loss / count_iter:.4f}\")\n",
    "                start_time = time.time()\n",
    "            torch.cuda.empty_cache()\n",
    "            pbar.update(1)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"==> Average Training loss: mse ={total_loss / len(dataloader)}\")\n",
    "    print(f\"==> Epoch Training Time: {format_time(epoch_time)}\")\n",
    "    print(f\"================================================================\\n\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader,loss_function, encode_mode='onehot_meiler', device= 'cuda', mode='valid'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    targets = []\n",
    "    binary_predictions = []\n",
    "    binary_targets = []\n",
    "    orig_lens = []\n",
    "\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_model = esm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    protbert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "    protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "\n",
    "            batch_sequences = batch['seq']\n",
    "            mask = batch['mask'].to(device)\n",
    "\n",
    "            ## different encoding here\n",
    "            if encode_mode == 'esm':\n",
    "                x = embed_esm_batch(batch_sequences, esm_model, alphabet).to(device)\n",
    "                x   = F.pad(x, (0, 0, 0, max(1000 - x.size(1), 0)))\n",
    "            elif encode_mode == 'protbert':\n",
    "                x = embed_protbert_batch(batch_sequences, protbert_model, protbert_tokenizer).to(device)\n",
    "                x   = F.pad(x, (0, 0, 0, max(1000 - x.size(1), 0)))\n",
    "            elif encode_mode == 'onehot':\n",
    "                x = onehot_encode_batch(batch_sequences,1000).to(device)\n",
    "            else:\n",
    "                x = onehot_meiler_encode_batch(batch_sequences,1000).to(device)\n",
    "\n",
    "            ## convert (bsz,max_len) to  (bsz,max_len,1)\n",
    "            y_reg = batch['target_reg'].unsqueeze(2).float().to(device)\n",
    "            y_bin = batch['target_bin'].unsqueeze(2).float().to(device)\n",
    "            \n",
    "            y_reg = clean_output_batch(y_reg, mask)\n",
    "            y_bin = clean_output_batch(y_bin, mask)\n",
    "\n",
    "            ### predict \n",
    "            final_info, output_reg = model(x, mask)\n",
    "            \n",
    "            #trim out the padded part\n",
    "            output_reg = clean_output_batch(output_reg, mask)\n",
    "            # print(orig_len.sum())\n",
    "            assert len(output_reg)==len(y_reg) , 'reg output {} and target {} not same length'.format(len(output_reg),len(y_reg))\n",
    "\n",
    "            # current_loss = reg_loss \n",
    "            current_loss = loss_function(output_reg, y_reg)\n",
    "     \n",
    "            total_loss += current_loss.item()\n",
    "\n",
    "            #append to list of all preds\n",
    "            predictions.append(output_reg.cpu().numpy())\n",
    "            targets.append(y_reg.cpu().numpy())\n",
    "\n",
    "            y_bin = (y_reg.cpu().numpy() > 0).astype(int)\n",
    "            out_bin = (output_reg.cpu().numpy() > 0).astype(int)\n",
    "\n",
    "            binary_predictions.append(out_bin)\n",
    "            binary_targets.append(y_bin)\n",
    "\n",
    "    # if mode == 'test':\n",
    "    all_predictions = np.concatenate(predictions, axis=0).reshape(-1)\n",
    "    all_targets = np.concatenate(targets, axis=0).reshape(-1)\n",
    "    all_binary_predictions = np.concatenate(binary_predictions, axis=0).reshape(-1)\n",
    "    all_binary_targets = np.concatenate(binary_targets, axis=0).reshape(-1)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_mse = mean_squared_error(all_targets, all_predictions)\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    overall_mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    overall_r2 = r2_score(all_targets, all_predictions)\n",
    "    overall_pcc, _ = pearsonr(all_targets.flatten(), all_predictions.flatten())\n",
    "\n",
    "    # Calculate binary classification metrics\n",
    "    overall_accuracy = accuracy_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_precision = precision_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_recall = recall_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_f1 = f1_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_auc_roc = roc_auc_score(all_binary_targets, all_predictions)\n",
    "    overall_auc_pr = average_precision_score(all_binary_targets, all_predictions)\n",
    "    overall_mcc = matthews_corrcoef(all_binary_targets, all_binary_predictions)\n",
    "\n",
    "    print(f\"Overall Regression Metrics\")\n",
    "    print(f\"MSE: {overall_mse:.4f}, RMSE: {overall_rmse:.4f}, MAE: {overall_mae:.4f}, R2: {overall_r2:.4f}, PCC: {overall_pcc:.4f}\")\n",
    "\n",
    "    print(f\"Overall classification Metrics\")\n",
    "    print(f\"Acc: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1-Score: {overall_f1:.4f}, AUC-ROC: {overall_auc_roc:.4f}, AUC-PR: {overall_auc_pr:.4f}, MCC: {overall_mcc:.4f}\")  \n",
    "    \n",
    "    metrics = {\n",
    "        \"Regression Metrics\": {\n",
    "            \"MSE\": round(float(overall_mse), 4),\n",
    "            \"RMSE\": round(float(overall_rmse), 4),\n",
    "            \"MAE\": round(float(overall_mae), 4),\n",
    "            \"R2\": round(float(overall_r2), 4),\n",
    "            \"PCC\": round(float(overall_pcc), 4)\n",
    "        },\n",
    "        \"Classification Metrics\": {\n",
    "            \"Accuracy\": round(float(overall_accuracy), 4),\n",
    "            \"Precision\": round(float(overall_precision), 4),\n",
    "            \"Recall\": round(float(overall_recall), 4),\n",
    "            \"F1-Score\": round(float(overall_f1), 4),\n",
    "            \"AUC-ROC\": round(float(overall_auc_roc), 4),\n",
    "            \"AUC-PR\": round(float(overall_auc_pr), 4),\n",
    "            \"MCC\": round(float(overall_mcc), 4)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    return total_loss / len(dataloader),metrics, predictions, targets\n",
    "\n",
    "def train_loop(model, optimizer, train_dataloader, valid_dataloader,loss_function, nb_epochs, encode_mode='onehot_meiler', device= 'cuda', save_directory='./weights/'):\n",
    "    start_epoch = 1\n",
    "    best_validation_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Paths for saving losses and metrics\n",
    "    loss_output_path = os.path.join(save_directory, 'losses.json')\n",
    "    metric_output_path = os.path.join(save_directory, 'metrics.json')\n",
    "    \n",
    "    # Initialize lists for losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        print(f'Created directory: {save_directory}')\n",
    "\n",
    "    checkpoint_path = os.path.join(save_directory, 'model_last.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        \n",
    "        # Load losses from the losses.json file if it exists\n",
    "        if os.path.exists(loss_output_path):\n",
    "            with open(loss_output_path, 'r') as f:\n",
    "                losses = json.load(f)\n",
    "                train_losses = losses.get('train_losses', [])\n",
    "                val_losses = losses.get('val_losses', [])\n",
    "            print(f'Loaded losses from {loss_output_path}.')\n",
    "            print(train_losses)\n",
    "            print(val_losses)\n",
    "        else:\n",
    "            print(f'No losses file found at {loss_output_path}.')\n",
    "\n",
    "    else:\n",
    "        print('No checkpoint found. Starting from beginning.')\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, nb_epochs + 1):\n",
    "        print(\"==================================================================================\")\n",
    "        print(f'                            -----EPOCH {epoch}-----')\n",
    "        print(\"==================================================================================\")\n",
    "        \n",
    "        train_loss = train_epoch(model, optimizer, train_dataloader,loss_function, encode_mode ,device, printEvery=1000)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(\"==========================VALIDATION===============================================\")\n",
    "        val_loss ,metrics, _ , _ = evaluate(model, valid_dataloader,loss_function,encode_mode, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'==> Epoch {epoch} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_validation_loss:\n",
    "            early_stopping_counter = 0\n",
    "            best_validation_loss = val_loss\n",
    "            best_model_save_path = os.path.join(save_directory, 'model_best.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'validation_accuracy': val_loss,\n",
    "            }, best_model_save_path)\n",
    "            print('\\n')\n",
    "            print(f'Best model checkpoint saved to: {best_model_save_path}')\n",
    "\n",
    "            # Save metrics of the best model\n",
    "            with open(metric_output_path, 'w') as json_file:\n",
    "                json.dump(metrics, json_file, indent=4)\n",
    "        \n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= 5:\n",
    "                print(\"\\n==> Early stopping triggered. No improvement in validation loss for 3 epochs.\")\n",
    "                break\n",
    "\n",
    "        last_model_save_path = os.path.join(save_directory, 'model_last.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'validation_accuracy': val_loss,\n",
    "        }, last_model_save_path)\n",
    "        print(f'Last epoch model saved to: {last_model_save_path}')\n",
    "\n",
    "        # Save updated losses to the JSON file\n",
    "        losses = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }\n",
    "        with open(loss_output_path, 'w') as json_file:\n",
    "            json.dump(losses, json_file, indent=4)\n",
    "        print(f'Losses updated and saved to: {loss_output_path}')\n",
    "        \n",
    "        print(\"==================================================================================\\n\")\n",
    "    \n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ./weights/seq/(onehot_meiler)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/model_last.pt. Resuming from epoch 16\n",
      "Loaded losses from ./weights/seq/(onehot_meiler)_(regloss)_(local_3block256dim)_(global_1layer128_4head)/losses.json.\n",
      "[0.6688351124676865, 0.5371009643697172, 0.5063865821814901, 0.4925348173276678, 0.483561003552228, 0.47253315967898213, 0.42953249032914337, 0.4248547785755734, 0.42157351925134257]\n",
      "[0.6093979679249428, 0.5327651400018383, 0.5141573870504225, 0.5462429934256786, 0.4919483875100677, 0.5662333429665178, 0.42681065403126384, 0.4370848157921353, 0.43982857908751516]\n",
      "==================================================================================\n",
      "                            -----EPOCH 16-----\n",
      "==================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 31/589 [00:07<02:14,  4.14batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 31, Time: 9 s, Remaining: 2m 45 s, Training Loss: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 62/589 [00:15<02:08,  4.11batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 62, Time: 7 s, Remaining: 1m 4 s, Training Loss: 0.4977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 93/589 [00:22<01:57,  4.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 93, Time: 7 s, Remaining: 38 s, Training Loss: 0.4953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 124/589 [00:29<01:49,  4.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 124, Time: 7 s, Remaining: 27 s, Training Loss: 0.4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 155/589 [00:37<01:44,  4.16batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 155, Time: 7 s, Remaining: 20 s, Training Loss: 0.4947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 162/589 [00:38<01:42,  4.17batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m combined_loss\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# loss_function = mse_loss\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencode_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 256\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, optimizer, train_dataloader, valid_dataloader, loss_function, nb_epochs, encode_mode, device, save_directory)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m                            -----EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==================================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 256\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintEvery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==========================VALIDATION===============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, loss_function, encode_mode, device, printEvery)\u001b[0m\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m onehot_encode_batch(batch_sequences,\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43monehot_meiler_encode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m## convert (bsz,max_len) to  (bsz,max_len,1)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m y_reg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/AIDRUG-AUDENSIEL/aggrepred/utils.py:118\u001b[0m, in \u001b[0;36monehot_meiler_encode_batch\u001b[0;34m(sequences, max_length)\u001b[0m\n\u001b[1;32m    116\u001b[0m batch_encoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size, max_length, NUM_AMINOS\u001b[38;5;241m+\u001b[39m NUM_MEILER))\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[0;32m--> 118\u001b[0m     batch_encoded[i] \u001b[38;5;241m=\u001b[39m \u001b[43monehot_meiler_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_encoded\n",
      "File \u001b[0;32m~/AIDRUG-AUDENSIEL/aggrepred/utils.py:102\u001b[0m, in \u001b[0;36monehot_meiler_encode\u001b[0;34m(sequence, max_length)\u001b[0m\n\u001b[1;32m    100\u001b[0m     aa \u001b[38;5;241m=\u001b[39m sequence[i]\n\u001b[1;32m    101\u001b[0m     encoded[i][aa2idx\u001b[38;5;241m.\u001b[39mget(aa, NUM_AMINOS\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 102\u001b[0m     encoded[i][\u001b[38;5;241m-\u001b[39mNUM_MEILER:] \u001b[38;5;241m=\u001b[39m MEILER[aa] \u001b[38;5;28;01mif\u001b[39;00m aa \u001b[38;5;129;01min\u001b[39;00m MEILER \u001b[38;5;28;01melse\u001b[39;00m MEILER[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(path, exist_ok=True)\n",
    "with open(os.path.join(path, \"config.json\"), 'w') as json_file:\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "\n",
    "loss_function = combined_loss\n",
    "# loss_function = mse_loss\n",
    "\n",
    "train_loop(model,optimizer,train_dataloader,valid_dataloader,loss_function, 50, config['encode_mode'],device,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on test set and return a result in json format for all the trained model above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_model_from_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Loads the model and optimizer state from a checkpoint if it exists.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to load the state into.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n",
    "    - checkpoint_path (str): Path to the checkpoint file.\n",
    "    - device (torch.device): Device to which the model should be moved.\n",
    "    \n",
    "    Returns:\n",
    "    - start_epoch (int): The epoch to start training from.\n",
    "    - best_validation_loss (float): The best validation loss recorded in the checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        # print(f'Best validation loss: {best_validation_loss}')\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_validation_loss = float('inf')  # Assuming lower is better for validation loss\n",
    "        print('No checkpoint found.')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return start_epoch, best_validation_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ./weights/seq/(onehot)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/model_best.pt. Resuming from epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2023/ly-an.chhay/.conda/envs/aggrepred/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Regression Metrics\n",
      "MSE: 0.5068, RMSE: 0.7119, MAE: 0.5218, R2: 0.7615, PCC: 0.8738\n",
      "Overall classification Metrics\n",
      "Acc: 0.8670, Precision: 0.6567, Recall: 0.6564, F1-Score: 0.6566, AUC-ROC: 0.9064, AUC-PR: 0.7137, MCC: 0.5741\n",
      "Processed model in path: ./weights/seq/(onehot)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\n",
      "\n",
      "Loaded checkpoint from ./weights/seq/(onehot_meiler)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/model_best.pt. Resuming from epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2023/ly-an.chhay/.conda/envs/aggrepred/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Regression Metrics\n",
      "MSE: 0.4048, RMSE: 0.6362, MAE: 0.4620, R2: 0.8095, PCC: 0.9002\n",
      "Overall classification Metrics\n",
      "Acc: 0.8696, Precision: 0.6309, Recall: 0.7863, F1-Score: 0.7001, AUC-ROC: 0.9276, AUC-PR: 0.7790, MCC: 0.6242\n",
      "Processed model in path: ./weights/seq/(onehot_meiler)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of model paths\n",
    "model_paths = [\n",
    "    \n",
    "    \"./weights/seq/(onehot)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    \"./weights/seq/(onehot_meiler)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "   \n",
    "    # \"./weights/seq/(esm35M)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights/seq/(esm35M)_(combinedloss)_(none)/\",\n",
    "\n",
    "    # \"./weights/seq/(protbert)_(combinedloss)_(local_3block256dim)_(global_1layer128_4head)/\",\n",
    "    # \"./weights/seq/(protbert)_(combinedloss)_(none)/\"\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "for path in model_paths:\n",
    "    # Load the config for the current model\n",
    "    with open(path + 'config.json', 'r') as json_file:\n",
    "        config = json.load(json_file)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Aggrepred(config)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    _, _ = load_model_from_checkpoint(model, optimizer, path + 'model_best.pt', device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, metrics, preds, tar = evaluate(model, test_dataloader,combined_loss, config['encode_mode'] ,device)\n",
    "\n",
    "    # Save metrics of the best model\n",
    "    with open(path + 'result.json', 'w') as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "\n",
    "    print(f\"Processed model in path: {path}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aggrepred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
