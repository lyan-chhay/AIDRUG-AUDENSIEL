{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# PyTorch and PyTorch Geometric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LayerNorm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as graphDataLoader\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "\n",
    "# Transformers and ESM\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import esm\n",
    "\n",
    "# Custom imports from aggrepred\n",
    "top_folder_path = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "sys.path.insert(0, top_folder_path)\n",
    "\n",
    "from aggrepred.graph_utils import *\n",
    "from aggrepred.graph_model import  EGNN_Model\n",
    "\n",
    "# Default values used in training\n",
    "NEIGHBOUR_RADIUS = 10\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score,\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    average_precision_score, \n",
    "    matthews_corrcoef\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process all pdb to graph file format .pt\n",
    "### make sure to store all PDB file in \"../data/pdb/\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir  = \"../data/\"\n",
    "pdb_dir = \"../data/pdb/\"\n",
    "graph_dir = \"../data/graph/\"\n",
    "\n",
    "df = pd.read_csv(data_dir+\"csv/data60_fixed_split.csv\")\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "    code = row[\"ID\"]\n",
    "    # chain = row.chain\n",
    "    # code = row[\"code\"]\n",
    "    \n",
    "    graph_file_path = os.path.join(graph_dir, f\"{code}.pt\")\n",
    "    pdb_path = pdb_dir+ f\"{code}.pdb\"\n",
    "    \n",
    "    # Skip processing if the graph file already exists\n",
    "    if os.path.exists(graph_file_path):\n",
    "        continue\n",
    "    \n",
    "    _ = process_pdb2graph(pdb_path,graph_file_path)\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, df, graph_dir):\n",
    "        self.data_frame = df.copy()\n",
    "        self.codes = self.data_frame['ID'].tolist()\n",
    "        self.graph_dir = graph_dir\n",
    "\n",
    "        # Check if all graph files exist and filter the codes list accordingly\n",
    "        self.codes = [code for code in self.codes if os.path.exists(f\"{self.graph_dir}/{code}.pt\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        code_id = self.codes[idx]\n",
    "        graph_path = f\"{self.graph_dir}/{code_id}.pt\"\n",
    "        graph_data = torch.load(graph_path)\n",
    "        return graph_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/users/eleves-b/2023/ly-an.chhay/main/data/csv/all.csv'\n",
    "graph_dir = \"../data/graph/\"\n",
    "\n",
    "df = pd.read_csv(\"../data/csv/data60_fixed_split.csv\")\n",
    "\n",
    "train_dataset = GraphDataset(df[df.split=='train'], graph_dir)\n",
    "valid_dataset = GraphDataset(df[df.split=='valid'], graph_dir)\n",
    "test_dataset = GraphDataset(df[df.split=='test'], graph_dir)\n",
    "\n",
    "train_dataset = GraphDataset(df[df.split=='train'].sample(frac=0.10, random_state=42), graph_dir)\n",
    "valid_dataset = GraphDataset(df[df.split=='valid'].sample(frac=0.10, random_state=42), graph_dir)\n",
    "test_dataset = GraphDataset(df[df.split=='test'], graph_dir)\n",
    "\n",
    "train_dataloader = graphDataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = graphDataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = graphDataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable_params, non_trainable_params\n",
    "\n",
    "num_feats = 20\n",
    "graph_hidden_layer_output_dims = [20,20,20]\n",
    "linear_hidden_layer_output_dims = [10,10]\n",
    "\n",
    "model = EGNN_Model(num_feats = num_feats,\n",
    "                       graph_hidden_layer_output_dims = graph_hidden_layer_output_dims,\n",
    "                       linear_hidden_layer_output_dims = linear_hidden_layer_output_dims)\n",
    "\n",
    "\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {trainable}\")\n",
    "print(f\"Number of non-trainable parameters: {non_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "x , coors, edge_index , batch , y = batch.x , batch.pos, batch.edge_index, batch.batch, batch.y\n",
    "# convert edge_index to adjacent matrix, as in EGNN take adj_mat\n",
    "x = x.unsqueeze(0)\n",
    "coors = coors.unsqueeze(0)\n",
    "edges = edge_index_to_adjacency_matrix(edge_index).unsqueeze(2).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of x:', x.size())\n",
    "print('size of coors:', coors.size())\n",
    "print('size of edges:', edges.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# PARAM\n",
    "# ----------------\n",
    "\n",
    "\n",
    "# Define the configuration dictionary with all the model parameters\n",
    "# path = \"./weights/graph/(esm8M)_(1EGNN)/\"\n",
    "path = \"./weights/graph/(onehot)_(3EGNN)/\"\n",
    "\n",
    "config = {\n",
    "    \"model\": 'EGNN',\n",
    "    \"num_feats\" : 20,\n",
    "    \"graph_hidden_layer_output_dims\" : [20,20,20],\n",
    "    \"linear_hidden_layer_output_dims\" : [20,10],\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 1,\n",
    "    \"nb_epochs\": 20,\n",
    "    \"encode_mode\" : 'onehot'\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------\n",
    "#  MODEL \n",
    "# ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = EGNN_Model(num_feats = config[\"num_feats\"],\n",
    "                       graph_hidden_layer_output_dims = config[\"graph_hidden_layer_output_dims\"],\n",
    "                       linear_hidden_layer_output_dims = config[\"linear_hidden_layer_output_dims\"])\n",
    "\n",
    "\n",
    "\n",
    "# ----------------\n",
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable_params, non_trainable_params\n",
    "\n",
    "print(path)\n",
    "print(model)\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {trainable}\")\n",
    "print(f\"Number of non-trainable parameters: {non_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "#   OPTIMIZER \n",
    "# ----------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1.0, lambda_bin=1.0, pos_weight=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_bin = lambda_bin\n",
    "        self.mse_loss = nn.MSELoss()  # Regression Loss (MSE)\n",
    "        \n",
    "        if pos_weight is not None:\n",
    "            # Binary Classification Loss (Weighted BCE with logits)\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        else:\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, outputs, regression_targets):\n",
    "        # Calculate regression loss\n",
    "        reg_loss = self.mse_loss(outputs, regression_targets)\n",
    "        \n",
    "        # Calculate binary classification loss\n",
    "        # Convert regression output to binary labels (logits) for classification\n",
    "        binary_targets = (regression_targets> 0).float()\n",
    "        bin_loss = self.bce_loss(outputs, binary_targets)\n",
    "        \n",
    "        # Combined weighted loss\n",
    "        total_loss = self.lambda_reg * reg_loss + self.lambda_bin * bin_loss\n",
    "        return total_loss\n",
    "\n",
    "combined_loss = CombinedLoss(lambda_reg=0.7, lambda_bin=0.3, pos_weight=4.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = \"../data/graph/\"\n",
    "df = pd.read_csv(\"../data/csv/data60_fixed_split.csv\")\n",
    "\n",
    "train_dataset = GraphDataset(df[df.split=='train'].sample(frac=0.10, random_state=42), graph_dir)\n",
    "valid_dataset = GraphDataset(df[df.split=='valid'].sample(frac=0.10, random_state=42), graph_dir)\n",
    "test_dataset = GraphDataset(df[df.split=='test'], graph_dir)\n",
    "\n",
    "train_dataloader = graphDataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = graphDataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = graphDataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, device, printEvery=50):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count_iter = 0\n",
    "    start_time = time.time()\n",
    "    epoch_start_time = start_time\n",
    "    accumulation_steps = 8\n",
    "\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_model = esm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    protbert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "    protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')\n",
    "\n",
    "\n",
    "    with tqdm(total=len(dataloader), desc='Training', unit='batch') as pbar:\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            x, coors, edge_index, batch, y = batch.x, batch.pos, batch.edge_index, batch.batch, batch.y\n",
    "            \n",
    "            batch_sequences = [onehot_to_sequence(x)]\n",
    "            \n",
    "            if config[\"encode_mode\"] == 'esm':\n",
    "                x = embed_esm_batch(batch_sequences, esm_model, alphabet).to(device)\n",
    "            elif config[\"encode_mode\"] == 'protbert':\n",
    "                x = embed_protbert_batch(batch_sequences, protbert_model, protbert_tokenizer).to(device)\n",
    "            else:\n",
    "                x = x.unsqueeze(0).to(device)\n",
    "\n",
    "                \n",
    "            if config['model']== 'EGNN':\n",
    "                \n",
    "                edge_index = edge_index.to(device)\n",
    "                coors = coors.unsqueeze(0).to(device)\n",
    "                edges = edge_index_to_adjacency_matrix(edge_index).unsqueeze(2).unsqueeze(0).to(device)\n",
    "                out = model(x, coors, edges).squeeze()\n",
    "            else:\n",
    "                x = x.squeeze(0).to(device)\n",
    "                edge_index = edge_index.to(device)\n",
    "                out = model(x, edge_index).squeeze()\n",
    "\n",
    "            current_loss = combined_loss(out, y.to(device))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            current_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += current_loss.item()\n",
    "            \n",
    "            count_iter += 1\n",
    "            if count_iter % printEvery == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / count_iter) * (len(dataloader) - count_iter)\n",
    "                print(f\"Iteration: {count_iter}, Time: {format_time(elapsed_time)}, Remaining: {format_time(remaining_time)}, Training Loss: {total_loss / count_iter:.4f}\")\n",
    "                start_time = time.time()\n",
    "            \n",
    "            #remove cache to save GPU\n",
    "            torch.cuda.empty_cache()\n",
    "            pbar.update(1)\n",
    "            \n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"==> Average Training loss: mse ={total_loss / len(dataloader)}\")\n",
    "    print(f\"==> Epoch Training Time: {format_time(epoch_time)}\")\n",
    "    print(f\"================================================================\\n\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, mode='valid'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    targets = []\n",
    "    binary_predictions = []\n",
    "    binary_targets = []\n",
    "\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    # esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_model = esm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    protbert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "    protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            x, coors, edge_index, batch, y = batch.x, batch.pos, batch.edge_index, batch.batch, batch.y\n",
    "            \n",
    "            batch_sequences = [onehot_to_sequence(x)]\n",
    "            \n",
    "            if config[\"encode_mode\"] == 'esm':\n",
    "                x = embed_esm_batch(batch_sequences, esm_model, alphabet).to(device)\n",
    "            elif config[\"encode_mode\"] == 'protbert':\n",
    "                x = embed_protbert_batch(batch_sequences, protbert_model, protbert_tokenizer).to(device)\n",
    "            else:\n",
    "                x = x.unsqueeze(0).to(device)\n",
    "            \n",
    "            if config['model']== 'EGNN':\n",
    "                \n",
    "                edge_index = edge_index.to(device)\n",
    "                coors = coors.unsqueeze(0).to(device)\n",
    "                edges = edge_index_to_adjacency_matrix(edge_index).unsqueeze(2).unsqueeze(0).to(device)\n",
    "                out = model(x, coors, edges).squeeze()\n",
    "            else:\n",
    "                x = x.squeeze(0).to(device)\n",
    "                edge_index = edge_index.to(device)\n",
    "                out = model(x, edge_index).squeeze()\n",
    "\n",
    "            current_loss = combined_loss(out, y.to(device))\n",
    "            # current_loss = weighted_bce_loss(out, (y>0).float().to(device)) + mse_loss(out, y.to(device))\n",
    "            \n",
    "            total_loss += current_loss.item()\n",
    "       \n",
    "\n",
    "            #append to list of all preds\n",
    "            predictions.append(out.cpu().numpy())\n",
    "            targets.append(y.cpu().numpy())\n",
    "            \n",
    "            ## Convert regression targets to binary labels\n",
    "            y_bin = (y.cpu().numpy() > 0).astype(int)\n",
    "            out_bin = (out.cpu().numpy() > 0).astype(int)\n",
    "            \n",
    "            binary_predictions.append(out_bin)\n",
    "            binary_targets.append(y_bin)\n",
    "\n",
    "    # if mode == 'test':\n",
    "    all_predictions = np.concatenate(predictions, axis=0)\n",
    "    all_targets = np.concatenate(targets, axis=0)\n",
    "    all_binary_predictions = np.concatenate(binary_predictions, axis=0)\n",
    "    all_binary_targets = np.concatenate(binary_targets, axis=0)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_mse = mean_squared_error(all_targets, all_predictions)\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    overall_mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    overall_r2 = r2_score(all_targets, all_predictions)\n",
    "    overall_pcc, _ = pearsonr(all_targets.flatten(), all_predictions.flatten())\n",
    "\n",
    "    # Calculate binary classification metrics\n",
    "    overall_accuracy = accuracy_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_precision = precision_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_recall = recall_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_f1 = f1_score(all_binary_targets, all_binary_predictions)\n",
    "    overall_auc_roc = roc_auc_score(all_binary_targets, all_predictions)\n",
    "    overall_auc_pr = average_precision_score(all_binary_targets, all_predictions)\n",
    "    overall_mcc = matthews_corrcoef(all_binary_targets, all_binary_predictions)\n",
    "\n",
    "    print(f\"Overall Reg Metrics - MSE: {overall_mse:.4f}, RMSE: {overall_rmse:.4f}, MAE: {overall_mae:.4f}, R2: {overall_r2:.4f}, PCC: {overall_pcc:.4f}\")\n",
    "    \n",
    "    print(f\"Overall Classification Metrics - Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1-Score: {overall_f1:.4f}, AUC-ROC: {overall_auc_roc:.4f}, AUC-PR: {overall_auc_pr:.4f}, MCC: {overall_mcc:.4f}\")\n",
    "    metrics = {\n",
    "        \"Regression Metrics\": {\n",
    "            \"MSE\": round(float(overall_mse), 4),\n",
    "            \"RMSE\": round(float(overall_rmse), 4),\n",
    "            \"MAE\": round(float(overall_mae), 4),\n",
    "            \"R2\": round(float(overall_r2), 4),\n",
    "            \"PCC\": round(float(overall_pcc), 4)\n",
    "        },\n",
    "        \"Classification Metrics\": {\n",
    "            \"Accuracy\": round(float(overall_accuracy), 4),\n",
    "            \"Precision\": round(float(overall_precision), 4),\n",
    "            \"Recall\": round(float(overall_recall), 4),\n",
    "            \"F1-Score\": round(float(overall_f1), 4),\n",
    "            \"AUC-ROC\": round(float(overall_auc_roc), 4),\n",
    "            \"AUC-PR\": round(float(overall_auc_pr), 4),\n",
    "            \"MCC\": round(float(overall_mcc), 4)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return total_loss / len(dataloader),metrics, predictions, targets\n",
    "\n",
    "def train_loop(model, optimizer, train_dataloader, valid_dataloader, nb_epochs, device, save_directory='./weights/'):\n",
    "    \n",
    "    start_epoch = 1\n",
    "    best_validation_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Paths for saving losses and metrics\n",
    "    loss_output_path = os.path.join(save_directory, 'losses.json')\n",
    "    metric_output_path = os.path.join(save_directory, 'metrics.json')\n",
    "    \n",
    "    # Initialize lists for losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        print(f'Created directory: {save_directory}')\n",
    "\n",
    "    checkpoint_path = os.path.join(save_directory, 'model_last.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print('No checkpoint found. Starting from beginning.')\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "    # Load existing losses if available\n",
    "    if os.path.exists(loss_output_path):\n",
    "        with open(loss_output_path, 'r') as json_file:\n",
    "            existing_losses = json.load(json_file)\n",
    "            train_losses = existing_losses.get('train_losses', [])\n",
    "            val_losses = existing_losses.get('val_losses', [])\n",
    "            print(train_losses)\n",
    "            print(val_losses)\n",
    "\n",
    "    for epoch in range(start_epoch, nb_epochs + 1):\n",
    "        print(\"==================================================================================\")\n",
    "        print(f'                            -----EPOCH {epoch}-----')\n",
    "        print(\"==================================================================================\")\n",
    "        \n",
    "        train_loss = train_epoch(model, optimizer, train_dataloader,device, printEvery=1000)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # # **Print Gradients**\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f'Gradient - {name}: {param.grad.norm()}')  # Prints the norm of gradients\n",
    "\n",
    "        print(\"==========================VALIDATION===============================================\")\n",
    "        val_loss ,metrics, _ , _ = evaluate(model, valid_dataloader,device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'==> Epoch {epoch} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_validation_loss:\n",
    "            early_stopping_counter = 0\n",
    "            best_validation_loss = val_loss\n",
    "            best_model_save_path = os.path.join(save_directory, 'model_best.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'validation_accuracy': val_loss,\n",
    "            }, best_model_save_path)\n",
    "            print('\\n')\n",
    "            print(f'Best model checkpoint saved to: {best_model_save_path}')\n",
    "\n",
    "            # Save metrics of the best model\n",
    "            with open(metric_output_path, 'w') as json_file:\n",
    "                json.dump(metrics, json_file, indent=4)\n",
    "        \n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= 3:\n",
    "                print(\"\\n==> Early stopping triggered. No improvement in validation loss for 3 epochs.\")\n",
    "                break\n",
    "\n",
    "        last_model_save_path = os.path.join(save_directory, 'model_last.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'validation_accuracy': val_loss,\n",
    "        }, last_model_save_path)\n",
    "        print(f'Last epoch model saved to: {last_model_save_path}')\n",
    "        print(\"==================================================================================\\n\")\n",
    "    \n",
    "        # Save updated losses to the JSON file\n",
    "        losses = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }\n",
    "        with open(loss_output_path, 'w') as json_file:\n",
    "            json.dump(losses, json_file, indent=4)\n",
    "        print(f'Losses updated and saved to: {loss_output_path}')\n",
    "        \n",
    "    return\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path, exist_ok=True)\n",
    "with open(os.path.join(path, \"config.json\"), 'w') as json_file:\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "model.to(device)\n",
    "train_loop(model,optimizer,train_dataloader,valid_dataloader,50, device,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load best model and test on test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_from_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Loads the model and optimizer state from a checkpoint if it exists.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to load the state into.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n",
    "    - checkpoint_path (str): Path to the checkpoint file.\n",
    "    - device (torch.device): Device to which the model should be moved.\n",
    "    \n",
    "    Returns:\n",
    "    - start_epoch (int): The epoch to start training from.\n",
    "    - best_validation_loss (float): The best validation loss recorded in the checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_validation_loss = checkpoint['validation_accuracy']\n",
    "        print(f'Loaded checkpoint from {checkpoint_path}. Resuming from epoch {start_epoch}')\n",
    "        # print(f'Best validation loss: {best_validation_loss}')\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_validation_loss = float('inf')  # Assuming lower is better for validation loss\n",
    "        print('No checkpoint found.')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return start_epoch, best_validation_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of model paths\n",
    "model_paths = [\n",
    "    \"./weights/graph/(onehot)_(3EGNN)/\",\n",
    "    \"./weights/graph/(esm8M)_(1EGNN)/\"\n",
    "]\n",
    "\n",
    "for path in model_paths:\n",
    "    # Load the config for the current model\n",
    "    with open(path + 'config.json', 'r') as json_file:\n",
    "        config = json.load(json_file)\n",
    "\n",
    "\n",
    "    # Initialize the model\n",
    "    model = EGNN_Model(num_feats = config[\"num_feats\"],\n",
    "                        graph_hidden_layer_output_dims = config[\"graph_hidden_layer_output_dims\"],\n",
    "                        linear_hidden_layer_output_dims = config[\"linear_hidden_layer_output_dims\"])\n",
    "\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    _, _ = load_model_from_checkpoint(model, optimizer, path + 'model_best.pt', device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, metrics, preds, tar = evaluate(model, test_dataloader ,device)\n",
    "\n",
    "    # Save metrics of the best model\n",
    "    with open(path + 'result.json', 'w') as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "\n",
    "    print(f\"Processed model in path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aggrepred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
