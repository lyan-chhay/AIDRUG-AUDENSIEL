{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "      <th>Hchain_sequence</th>\n",
       "      <th>Lchain_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADI-38502</td>\n",
       "      <td>66.0</td>\n",
       "      <td>train</td>\n",
       "      <td>EVQLLESGGGLVKPGGSLRLSCAASGFIFSDYSMNWVRQAPGKGLE...</td>\n",
       "      <td>DIVMTQSPSTLSASVGDRVTITCRASQSISSWLAWYQQKPGKAPKL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADI-38501</td>\n",
       "      <td>64.5</td>\n",
       "      <td>train</td>\n",
       "      <td>EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYSMNWVRQAPGKGLE...</td>\n",
       "      <td>DIVMTQSPATLSLSPGERATLSCRASQSISTYLAWYQQKPGQAPRL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADI-47173</td>\n",
       "      <td>64.5</td>\n",
       "      <td>train</td>\n",
       "      <td>EVQLVESGGGVVQPGRSLRLSCAASGFTFDRYGMHWIRQAPGKGLE...</td>\n",
       "      <td>EIVMTQSPLSLPVTLGQPASISCRSNQSLVHSDGNTFLNWFHQRPG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADI-47054</td>\n",
       "      <td>71.0</td>\n",
       "      <td>train</td>\n",
       "      <td>EVQLVESGPGLVKPSETLSLTCTVSGGSISSYYWSWIRQPPGKGLD...</td>\n",
       "      <td>QPVLTQPPSVSVSPGQTARITCSGDALPKQFVYWYQQTPRQAPVLV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADI-47278</td>\n",
       "      <td>71.5</td>\n",
       "      <td>train</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKASGYSFSSYGMHWVRQAPGQRLE...</td>\n",
       "      <td>EIVMTQSPATLSVSPGERATLSCRASQSVSTNLAWYQQKPGQAPRL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>urelumab</td>\n",
       "      <td>66.0</td>\n",
       "      <td>holdout</td>\n",
       "      <td>QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQSPEKGLE...</td>\n",
       "      <td>EIVLTQSPATLSLSPGERATLSCRASQSVSSYLAWYQQKPGQAPRL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>veltuzumab</td>\n",
       "      <td>70.0</td>\n",
       "      <td>holdout</td>\n",
       "      <td>QVQLQQSGAEVKKPGSSVKVSCKASGYTFTSYNMHWVKQAPGQGLE...</td>\n",
       "      <td>DIQLTQSPSSLSASVGDRVTMTCRASSSVSYIHWFQQKPGKAPKPW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>visilizumab</td>\n",
       "      <td>71.0</td>\n",
       "      <td>train</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKASGYTFISYTMHWVRQAPGQGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCSASSSVSYMNWYQQKPGKAPKRL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>zalutumumab</td>\n",
       "      <td>72.5</td>\n",
       "      <td>train</td>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFTFSTYGMHWVRQAPGKGLE...</td>\n",
       "      <td>AIQLTQSPSSLSASVGDRVTITCRASQDISSALVWYQQKPGKAPKL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>zanolimumab</td>\n",
       "      <td>80.5</td>\n",
       "      <td>test</td>\n",
       "      <td>QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQDISSWLAWYQHKPGKAPKL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  target    split  \\\n",
       "0      ADI-38502    66.0    train   \n",
       "1      ADI-38501    64.5    train   \n",
       "2      ADI-47173    64.5    train   \n",
       "3      ADI-47054    71.0    train   \n",
       "4      ADI-47278    71.5    train   \n",
       "..           ...     ...      ...   \n",
       "478     urelumab    66.0  holdout   \n",
       "479   veltuzumab    70.0  holdout   \n",
       "480  visilizumab    71.0    train   \n",
       "481  zalutumumab    72.5    train   \n",
       "482  zanolimumab    80.5     test   \n",
       "\n",
       "                                       Hchain_sequence  \\\n",
       "0    EVQLLESGGGLVKPGGSLRLSCAASGFIFSDYSMNWVRQAPGKGLE...   \n",
       "1    EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYSMNWVRQAPGKGLE...   \n",
       "2    EVQLVESGGGVVQPGRSLRLSCAASGFTFDRYGMHWIRQAPGKGLE...   \n",
       "3    EVQLVESGPGLVKPSETLSLTCTVSGGSISSYYWSWIRQPPGKGLD...   \n",
       "4    QVQLVQSGAEVKKPGASVKVSCKASGYSFSSYGMHWVRQAPGQRLE...   \n",
       "..                                                 ...   \n",
       "478  QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQSPEKGLE...   \n",
       "479  QVQLQQSGAEVKKPGSSVKVSCKASGYTFTSYNMHWVKQAPGQGLE...   \n",
       "480  QVQLVQSGAEVKKPGASVKVSCKASGYTFISYTMHWVRQAPGQGLE...   \n",
       "481  QVQLVESGGGVVQPGRSLRLSCAASGFTFSTYGMHWVRQAPGKGLE...   \n",
       "482  QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLE...   \n",
       "\n",
       "                                       Lchain_sequence  \n",
       "0    DIVMTQSPSTLSASVGDRVTITCRASQSISSWLAWYQQKPGKAPKL...  \n",
       "1    DIVMTQSPATLSLSPGERATLSCRASQSISTYLAWYQQKPGQAPRL...  \n",
       "2    EIVMTQSPLSLPVTLGQPASISCRSNQSLVHSDGNTFLNWFHQRPG...  \n",
       "3    QPVLTQPPSVSVSPGQTARITCSGDALPKQFVYWYQQTPRQAPVLV...  \n",
       "4    EIVMTQSPATLSVSPGERATLSCRASQSVSTNLAWYQQKPGQAPRL...  \n",
       "..                                                 ...  \n",
       "478  EIVLTQSPATLSLSPGERATLSCRASQSVSSYLAWYQQKPGQAPRL...  \n",
       "479  DIQLTQSPSSLSASVGDRVTMTCRASSSVSYIHWFQQKPGKAPKPW...  \n",
       "480  DIQMTQSPSSLSASVGDRVTITCSASSSVSYMNWYQQKPGKAPKRL...  \n",
       "481  AIQLTQSPSSLSASVGDRVTITCRASQDISSALVWYQQKPGKAPKL...  \n",
       "482  DIQMTQSPSSVSASVGDRVTITCRASQDISSWLAWYQHKPGKAPKL...  \n",
       "\n",
       "[483 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/csv/tm.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_esm_batch(batch_sequences, model, alphabet, pooling= False, repr_layer='last'):\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    data = [(\"protein\" + str(i), seq) for i, seq in enumerate(batch_sequences)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
    "\n",
    "    # Get the embeddings from the last layer\n",
    "    last_layer = model.num_layers\n",
    "    token_embeddings = results[\"representations\"][last_layer]\n",
    "\n",
    "   # If pooling is enabled, return mean-pooled embeddings for each protein\n",
    "    if pooling:\n",
    "        sequence_representations_list = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            # Pool by averaging over the tokens (excluding padding and special tokens)\n",
    "            sequence_representation = token_embeddings[i, 1:tokens_len - 1].mean(0)  # Exclude [CLS] and [SEP]\n",
    "            sequence_representations_list.append(sequence_representation)\n",
    "        # Return the pooled protein-level embeddings\n",
    "        return torch.stack(sequence_representations_list)\n",
    "\n",
    "    # Otherwise, return the full token-level embeddings (excluding special tokens)\n",
    "    return token_embeddings[:, 1:-1, :]\n",
    "\n",
    "def embed_protbert_batch(sequences, model, tokenizer, device='cuda' ):\n",
    "    model.eval()\n",
    "\n",
    "    sequences_w_spaces = [' '.join(list(seq)) for seq in sequences]\n",
    "    processed_sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_w_spaces]\n",
    "\n",
    "    ids = tokenizer.batch_encode_plus(processed_sequences, add_special_tokens=True, pad_to_max_length=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)[0]\n",
    "\n",
    "    return embedding[:,1:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t36_3B_UR50D.pt\" to /users/eleves-b/2023/ly-an.chhay/.cache/torch/hub/checkpoints/esm2_t36_3B_UR50D.pt\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load ESM-2 model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# esm.pretrained.esm2_t48_15B_UR50D() \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# esm.pretrained.esm2_t36_3B_UR50D() \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# esm.pretrained.esm2_t33_650M_UR50D()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model, alphabet \u001b[38;5;241m=\u001b[39m \u001b[43mesm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm2_t36_3B_UR50D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m batch_converter \u001b[38;5;241m=\u001b[39m alphabet\u001b[38;5;241m.\u001b[39mget_batch_converter()\n",
      "File \u001b[0;32m~/.conda/envs/aggrepred/lib/python3.10/site-packages/esm/pretrained.py:387\u001b[0m, in \u001b[0;36mesm2_t36_3B_UR50D\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mesm2_t36_3B_UR50D\u001b[39m():\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"36 layer ESM-2 model with 3B params, trained on UniRef50.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Returns a tuple of (Model, Alphabet).\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_and_alphabet_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mesm2_t36_3B_UR50D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/aggrepred/lib/python3.10/site-packages/esm/pretrained.py:63\u001b[0m, in \u001b[0;36mload_model_and_alphabet_hub\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_and_alphabet_hub\u001b[39m(model_name):\n\u001b[0;32m---> 63\u001b[0m     model_data, regression_data \u001b[38;5;241m=\u001b[39m \u001b[43m_download_model_and_regression_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_and_alphabet_core(model_name, model_data, regression_data)\n",
      "File \u001b[0;32m~/.conda/envs/aggrepred/lib/python3.10/site-packages/esm/pretrained.py:54\u001b[0m, in \u001b[0;36m_download_model_and_regression_data\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_model_and_regression_data\u001b[39m(model_name):\n\u001b[1;32m     53\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://dl.fbaipublicfiles.com/fair-esm/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 54\u001b[0m     model_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_hub_workaround\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_regression_weights(model_name):\n\u001b[1;32m     56\u001b[0m         regression_data \u001b[38;5;241m=\u001b[39m load_regression_hub(model_name)\n",
      "File \u001b[0;32m~/.conda/envs/aggrepred/lib/python3.10/site-packages/esm/pretrained.py:33\u001b[0m, in \u001b[0;36mload_hub_workaround\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_hub_workaround\u001b[39m(url):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Pytorch version issue - see https://github.com/pytorch/pytorch/issues/43106\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         fn \u001b[38;5;241m=\u001b[39m Path(url)\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/.conda/envs/aggrepred/lib/python3.10/site-packages/torch/hub.py:760\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    758\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    759\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001b[0;32m~/.conda/envs/aggrepred/lib/python3.10/site-packages/torch/hub.py:661\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    658\u001b[0m             sha256\u001b[38;5;241m.\u001b[39mupdate(buffer)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[1;32m    659\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(buffer))\n\u001b[0;32m--> 661\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hash_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     digest \u001b[38;5;241m=\u001b[39m sha256\u001b[38;5;241m.\u001b[39mhexdigest()  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "### note: loading the ESM 15 Billion parameters takes longer\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "# esm.pretrained.esm2_t48_15B_UR50D() \n",
    "# esm.pretrained.esm2_t36_3B_UR50D() \n",
    "# esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "#model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "sequence_representations_list = []\n",
    "save_list = []\n",
    "chunk_size = 25\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    H_batch = df[i:i+chunk_size][\"Hchain_sequence\"].tolist()\n",
    "    L_batch = df[i:i+chunk_size][\"Lchain_sequence\"].tolist()\n",
    "    ID_batch = df[i:i+chunk_size][\"ID\"].tolist()\n",
    "    target_batch = df[i:i+chunk_size][\"target\"].tolist()\n",
    "    split_batch = df[i:i+chunk_size][\"split\"].tolist()\n",
    "    \n",
    "    print(i+chunk_size)\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        H_out = embed_esm_batch(H_batch,model, alphabet,True)\n",
    "        L_out = embed_esm_batch(L_batch,model, alphabet,True)\n",
    "\n",
    "        sequence_representations = (H_out + L_out) / 2 \n",
    "     \n",
    "    for ID, target,split, embedding in zip(ID_batch, target_batch,split_batch, sequence_representations):\n",
    "\n",
    "        embedding_list = embedding.cpu().numpy().flatten().tolist()\n",
    "        \n",
    "        save_list.append({\n",
    "            \"ID\": ID,\n",
    "            \"target\": target,\n",
    "            \"split\": split,\n",
    "            \"embedding\": embedding_list\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_save = pd.DataFrame(save_list)\n",
    "\n",
    "# Save to CSV, embedding as a string\n",
    "# df_save.to_csv(\"tm_esm650M.csv\", index=False)\n",
    "df_save.to_csv(\"tm_esm3B.csv\", index=False)\n",
    "# df_save.to_csv(\"tm_esm650M.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aggrepred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
